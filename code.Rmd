---
title: "PAC 2 Regresión Lineal"
author: "Maria Lucas"
date: "2023-06-22"
output: 
  word_document:
    toc: true
    toc_depth: 4
lang: Es-es
---

Nota: Todos los códigos completos y el resultado de su ejecución se pueden encontrar en el ANEXO.

\newpage

# Ejercicio 1

Se ha procedido a la carga de los datos, asegurándonos de asignar el estatus de factor a las variables "diagnosis" y "age_cat" (Código 1).

```{r}
# Código 1: Carga de datos ejercicio 1

# Set viasulization number for results
options(scipen = 999)

# Set the file path
file_path <- "D:/Antiguos estudios/MASTER2/Sem2/Regresion/PAC2/P2/pancreas_biomarkers.txt"

# Load the data into a data frame
data <- read.table(file_path, header = TRUE, sep = "\t")

# Saving variables as factors
data$diagnosis <- factor(data$diagnosis, levels = 1:3)
data$age_cat <- factor(data$age_cat)
```

### (a) Modelo de regresión logística

Dado que nuestro objetivo principal consiste en distinguir entre pacientes diagnosticados con cáncer y aquellos con otras afecciones pancreáticas, se excluyeron del modelo los individuos que pertenecen al grupo de control. A continuación, se realizó un ajuste de regresión utilizando "diagnosis" como variable dependiente y las variables independientes "age_cat", "creatinina", "LYVE1", "REG1B" y "TFF1" (Código 2).

Aunque la inclusión de pacientes control en este caso no proporcionaría información específica sobre las diferencias entre los grupos de pacientes con cáncer y aquellos con afecciones no cancerosas, su exclusión del estudio plantea ciertas limitaciones. Entre estas limitaciones se destaca la posible falta de generalización de los resultados a la población general, así como la dificultad para evaluar la sensibilidad o especificidad del modelo.

Es importante tener en cuenta que al aplicar el modelo, se debe hacerlo con precaución y limitarlo a casos en los que se tenga conocimiento de una afectación pancreática, pero no se disponga de información sobre si es de naturaleza cancerosa. No se deben incluir pacientes sanos en la aplicación del modelo, ya que este no ha sido construido con los datos necesarios para tal propósito.

```{r}
# Código 2: Ajuste regresión logística ejercicio 1a

# Subset the data to include only levels 2 and 3 of the "diagnosis" variable
subset_data <- subset(data, diagnosis != 1)

# Recode values 2 and 3 to 0 and 1, respectively
subset_data$diagnosis <- ifelse(subset_data$diagnosis == 2, 0, 1)

# Fit the logistic regression model using the subsetted data
model <- glm(diagnosis ~ age_cat + creatinine + LYVE1 + REG1B + TFF1, data = subset_data, family = binomial)

# View the model summary
summary(model)
```

Los resultados obtenidos revelan que la edad, a partir de los 56 años, constituye un indicador significativo. Asimismo, se destaca que tanto LYVE1 como REG1B son variables relevantes para la predicción del riesgo de adenocarcinoma ductal pancreático. Esto se evidencia en el hecho de que todas estas variables presentan un valor p inferior a 0.05. En el contexto de la regresión logística, un valor p de 0.05 permite rechazar la hipótesis nula de ausencia de relación entre la variable predictora y la variable respuesta. En otras palabras, dichas variables tienen un impacto significativo sobre la clase.

Es importante resaltar que la significancia del intercepto sugiere que parte de la variabilidad de la variable respuesta no puede ser explicada por las variables independientes consideradas en nuestro estudio. En la regresión logística, el intercepto captura la probabilidad de que ocurra un evento cuando todas las variables predictoras se encuentran en su nivel de referencia. En otras palabras, la significancia del intercepto implica que existen diferencias significativas entre las clases, incluso en ausencia de cualquier predictor.

En conclusión, la edad (especialmente a partir de los 56 años), LYVE1 y REG1B son las variables que permiten predecir el riesgo de cáncer pancreático, brindando así la capacidad de distinguir entre otras afecciones pancreáticas no cancerosas.

### (b) Interpretación de coeficientes

En un modelo de regresión logística, los coeficientes indican el cambio estimado en el logaritmo de las probabilidades (log-odds) del evento en estudio (en este caso, la presencia de adenocarcinoma ductal pancreático, codificado como 1) asociado a un cambio unitario en la variable predictora, manteniendo constantes las demás variables.

Un coeficiente estimado positivo sugiere que un incremento en el valor de la variable está asociado con un aumento en la probabilidad (log-odds) del evento en estudio. Por ejemplo, un coeficiente de 0.31 indica que un incremento de 1 en el valor del predictor se asocia con un aumento de 0.31 en el log-odds del evento en estudio. Este es el caso de LYVE1, donde un aumento en el valor de LYVE1 está relacionado con una mayor probabilidad de tener adenocarcinoma (0.3140). De manera similar, la edad también parece ser un factor influyente. Tener más de 56 años incrementa el log-odds de tener adenocarcinoma, y dicho incremento varía según los rangos de edad: de 56 a 65 años (3.032), de 66 a 75 años (2.7) y a partir de los 75 años (3.443).

A simple vista, podría parecer que la edad tiene un impacto mayor en la probabilidad de presentar adenocarcinoma en comparación con LYVE1 (0.31 < 2.7-3.4). Sin embargo, es importante tener en cuenta que esta comparación se basa en un aumento de 1 en el valor de la edad o LYVE1. La interpretación de la magnitud de los coeficientes estimados debe realizarse con cautela, ya que depende de la escala y las unidades de medida de cada variable. Además, es importante considerar que la escala de las variables puede influir en la magnitud de los coeficientes estimados y, por lo tanto, debe tenerse en cuenta al interpretarlos.

Por otro lado, un coeficiente estimado negativo indica que un aumento en el valor de la variable está asociado con una disminución en el log-odds del evento en estudio (tener adenocarcinoma).

Con el propósito de mejorar la interpretación de los coeficientes, se ha aplicado una transformación al odds ratio mediante el cálculo exponencial (Código 3). El odds ratio representa la multiplicación de las probabilidades de éxito al comparar dos grupos que difieren en una unidad en el predictor numérico. Un odds ratio mayor a 1 indica que el aumento en el predictor numérico está asociado con un mayor riesgo relativo de ocurrencia del evento de interés. Por otro lado, un odds ratio menor a 1 indica un menor riesgo relativo de ocurrencia del evento, mientras que un odds ratio de 1 indica que no hay cambio en el riesgo relativo entre los dos grupos comparados.

```{r}
# Código 3: Conversión de log-odds a odds ratio para el ejercicio 1b

# Extract the coefficient values
coef_values <- data.frame(coef(model))

# Exponentiate the coefficients to obtain odds ratios
odds_ratios <- exp(coef_values)

# Format the table using kable()
library(knitr)
formatted_table <- kable(odds_ratios, align = "c", col.names = c("Odds ratios"), format = "pandoc")

# Print the odds ratios
print(formatted_table)
```

Al examinar los valores del odds ratio, se evidencia que a medida que la edad aumenta, parece existir una mayor probabilidad de presentar adenocarcinoma (con un odds ratio más alto). Sin embargo, es importante destacar que esta relación no se aplica a la categoría de edad de 66 a 75 años, donde la probabilidad es menor en comparación con la categoría de 56 a 65 años. Por otro lado, el predictor LYVE1 también muestra un aumento en la probabilidad de padecer adenocarcinoma.

Es crucial tener en cuenta que, dado que los odds ratios son medidas relativas, no es apropiado realizar una comparación directa de los valores de los odds ratios de dos variables diferentes para determinar cuál tiene un mayor impacto en la probabilidad del evento en cuestión. Esto se debe a que la magnitud de los odds ratios puede variar según el rango y la escala de las variables involucradas.

### (c) Modelo reducido

Para realizar una comparación entre ambos modelos, podemos aplicar un análisis de varianza (ANOVA) utilizando el test de Chi cuadrado (Código 4). Con esto, evaluamos el ajuste de cada modelo bajo las siguientes hipótesis:

- **H0:** Desviación del modelo reducido = Desviación del modelo completo. No hay diferencia de ajuste entre los modelos.
- **H1:** Desviación del modelo reducido > Desviación del modelo completo. El modelo reducido presenta un ajuste deficiente en comparación con el modelo completo.

La desviación del modelo es una medida de la discrepancia entre los valores observados de los datos y los valores predichos por el modelo.

```{r}
# Código 4: Anova del modelo reducido vs completo, ejercicio 1c 

# Fit the logistic regression model using the subsetted data and indicated variables
model_simple <- glm(diagnosis ~ age_cat + LYVE1 + REG1B, data = subset_data, family = binomial)

summary(model_simple)

# Compare the two models
anova(model, model_simple, test = "Chi")
```

Al observar el resultado del p-valor (0.31), no encontramos evidencia significativa en contra de la hipótesis nula. Por lo tanto, aceptamos la hipótesis nula y concluimos que el modelo reducido (sin creatinina y sin TFF1) tiene un ajuste comparable al modelo completo. Además, al comparar los valores del criterio de información de Akaike (AIC), observamos que el AIC del modelo reducido es 2 unidades menor que el del modelo completo. Esto sugiere que el modelo reducido tiene un mejor ajuste, teniendo en cuenta la complejidad de ambos modelos.

### (d) Funcion cuadrática

Ajustamos dos modelos adicionales, uno que incluye el término cuadrático de LYVE1 y otro que incluye el término cuadrático de REB1B. Posteriormente, realizamos un análisis de varianza (ANOVA) para comparar cada modelo con su versión reducida (sin los términos cuadráticos) (Código 5).

```{r}
# Codigo 5: Suposición de linealidad mediante la adición de términos cuadráticos, ejercicio 1d

# Cuadratic LYVE1
model_simple_LYVE1 = glm(diagnosis ~ age_cat + LYVE1 + I(LYVE1^2) + REG1B, data = subset_data, family = binomial)

summary(model_simple_LYVE1)

# Compare the two models
anova(model_simple, model_simple_LYVE1, test = "Chi")

# Cuadratic REG1B
model_simple_REG1B = glm(diagnosis ~ age_cat + LYVE1 + REG1B + I(REG1B^2), data = subset_data, family = binomial)

summary(model_simple_REG1B)

# Compare the two models
anova(model_simple, model_simple_REG1B, test = "Chi")
```

En ambos casos, al añadir los términos cuadráticos, no se observó una mejora significativa en el ajuste del modelo. Siguiendo la explicación anterior, los valores de p fueron 0.3 y 0.4, respectivamente, lo que nos lleva a aceptar la hipótesis nula en ambos casos. Esto implica que los modelos con y sin los términos cuadráticos tienen un ajuste similar.

Además, al analizar la desviación de cada modelo durante la realización de la prueba de anova, se observa que no disminuye de manera significativa. Este hallazgo sugiere que la incorporación de los términos cuadráticos no mejora el ajuste del modelo, lo que a su vez indica que no existen indicios de no-linealidad.

En conclusión, no se recomienda incluir ninguno de los términos cuadráticos en los modelos. Es importante destacar que, aunque los resultados de los tests no sean significativos, siempre se deben evaluar en conjunto con el contexto del análisis. En este caso, se realizó una inspección visual adicional de la relación entre las variables y el log-odds de la variable respuesta (disponible en el ANEXO, Código 6). Estos gráficos confirman que existe una relación lineal, ya que se observa un patrón lineal sin la presencia de curvas, forma de U u otros patrones no lineales.

```{r}
# Código 6: variable vs log odds para linealidad del ejercicio 1d

# Obtain predicted log odds from the model
predicted_logodds <- predict(model_simple, type = "link")

plot(subset_data$LYVE1, predicted_logodds, xlab = "age_cat", ylab = "Log Odds", main = "Scatter plot - LYVE1 vs. Log Odds")

plot(subset_data$REG1B, predicted_logodds, xlab = "age_cat", ylab = "Log Odds", main = "Scatter plot - REG1B vs. Log Odds")
```

### (e) Predicción de caso

Guardamos los datos del paciente con los valores indicados en el enunciado. Dado que la edad del paciente es de 68 años, esta cae dentro del rango de 66 a 75 años, por lo que se almacena en la variable "age_cat" con dicho rango. El resto de las variables se guardan utilizando los valores numéricos correspondientes. A continuación, aplicamos la función "predict" al modelo reducido utilizando estos datos y especificamos el tipo de respuesta como "response" para obtener la probabilidad con la que el caso ha sido clasificado (Código 7).

```{r}
# Código 7: Predicción de caso para el ejercicio 1e

# Create a new data frame for the new case
new_case = data.frame(age_cat = "66-75", LYVE1 = 6, REG1B = 140)

# Predict the outcome using the model
prediction = predict(model_simple, newdata = new_case, type = "response")

cat("Tipo de afección: ", names(prediction), "\n")
cat("Probabilidad de la clasificación: ", prediction[1])
```
El modelo predice la presencia de adenocarcinoma ductal pancreático con una probabilidad del 72,42%. Cabe destacar que los valores fueron recodificados de la siguiente manera: 0 representa afecciones pancreáticas no cancerosas y 1 representa adenocarcinoma ductal pancreático.

Es importante tener en cuenta que la extrapolación se produce cuando se realizan predicciones para datos de la variable predictora que se encuentran fuera del rango de los datos utilizados para construir el modelo. En este caso, es necesario considerar que la predicción se basa en los datos y el rango utilizados durante el entrenamiento del modelo, por lo que las predicciones para valores fuera de ese rango pueden ser menos precisas o no reflejar adecuadamente la realidad.

Con el propósito de verificar la idoneidad del modelo, hemos examinado los rangos de las variables LYVE1 y REG1B. En el código implementado, se ha incorporado una comparación automática que determina si los valores pertenecen o no a los rangos establecidos, utilizando un valor booleano. Además, se han generado gráficos que representan los datos utilizados en la construcción del modelo para cada variable, resaltando en color rojo el caso de estudio en cuestión (Código 8).

```{r}
# Código 8: Comprobación de extrapolación para el caso del ejercicio 1e

# Check predictor variable ranges
range_data <- sapply(subset_data[, c("LYVE1", "REG1B")], range)
range_new_observation <- c(6, 140)  # Replace with the values of the new observation

# Compare new observation values with observed range
is_extrapolation <- any(range_new_observation < range_data[1, ] | range_new_observation > range_data[2, ])

# Print results
cat("Extrapolación:", is_extrapolation, "\n")

# Assess distribution of predictor variables
# LYVE1
hist(subset_data$LYVE1, main = "Distribution of LYVE1")
# Add new observation to LYVE1 plot
points(6, 0, col = "red", pch = 16)

# REGB1
hist(subset_data$REG1B, main = "Distribution of REG1B")
# Add new observation to REGB1 plot
points(140, 0, col = "red", pch = 16)

# age_cat
barplot(table(subset_data$age_cat), main = "Distribution of age_cat", xlab = "age_cat", ylab = "Frequency")
# Add new observation to age_cat plot
points(5.5, 0, col = "red", pch = 16)
```

Como podemos observar, los valores correspondientes al caso que deseamos predecir se encuentran dentro de los rangos utilizados en la construcción del modelo para las tres variables.

Es relevante destacar que las variables creatinina y TFF1 no han sido incluidas en el modelo debido a que hemos determinado que el modelo reducido, es decir, aquel sin la inclusión de estas variables, presenta un ajuste igual o mejor que el modelo completo. Del mismo modo, podemos aplicar el modelo en este caso particular, dado que se trata de un paciente con una afección pancreática que requerimos clasificar como cancerosa o no cancerosa. Como hemos explicado previamente, si se tratara de un paciente sano, no podríamos aplicar este modelo con la misma certeza.

# Ejercicio 2

Se procede a realizar la carga de los datos utilizando el código proporcionado en el enunciado. Se ha optado por cambiar los nombres de las variables, ya que la presencia de paréntesis podría ocasionar problemas durante la ejecución del código. A continuación, se lleva a cabo la creación de un data frame que incluye únicamente las variables de interés. Se realiza el proceso de eliminación de valores perdidos y, utilizando el paquete "caret", se procede a la división de los datos en conjuntos de entrenamiento (80%) y prueba (20%) (Código 9).

```{r}
# Código 9: Carga de datos para el ejercicio 2 y 3

set.seed(123) # Seed is fixed multiple times in the code because it somehow was needed

# Data import
# Note: I changed the variable names to avoid problems with symbols

import.data <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data"
data <- read.table(url(import.data), sep=",", skip=1)
names(data) <- c("subject#","age","sex","test_time","motor_UPDRS","total_UPDRS",
"Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")

# Select predictor variables and response
library(dplyr)
parkinson <- data %>% dplyr::select(6:22)

# Delete NA values
parkinson <- na.omit(parkinson)

# Split the data into train and test
library(caret)
set.seed(123)
train_indices <- createDataPartition(parkinson$total_UPDRS, p = 0.8, list = FALSE)
data.train <- parkinson[train_indices, ]
data.test <- parkinson[-train_indices, ]
```

### (a) Regresión lineal

Se procede al ajuste del modelo lineal con la variable "total_UPDRS" como variable dependiente y las 16 variables indicadas como independientes. Se extrae la R2 y R2 ajustada del modelo. Posteriormente se calculan las predicciones sobre los datos de entrenamiento y prueba separadamente, se calculan los residuos y finalmente el RMSE (Código 10). Se presentan las métricas calculadas en una tabla.

Es importante resaltar que, a lo largo de este informe, nos referiremos exclusivamente al coeficiente R2 calculado a partir de los datos de entrenamiento y no a partir de los datos de prueba. El cálculo del R2 utilizando los datos de entrenamiento nos proporciona información sobre cómo el modelo se ajusta a los datos utilizados para su entrenamiento. Por otro lado, el cálculo del R2 utilizando los datos de prueba nos brinda una medida de qué tan bien el modelo generaliza y se desempeña en nuevos datos.

```{r}
# Código 10: Ajuste de modelo lineal, ejercicio 2a

# Fit the model
model_lineal = lm(total_UPDRS ~ ., data = data.train)

# Extract R-squared
r_squared = summary(model_lineal)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_lineal)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_lineal, data.train)
# Predict on the test data
predictions_test <- predict(model_lineal, data.test)

# Calculate residuals train
residuals_train <- predictions_train - data.train$total_UPDRS
residuals_test <- predictions_test - data.test$total_UPDRS

# Calculate RMSE train
lineal_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
lineal_rmse_test <- sqrt(mean(residuals_test^2))

# Create results table
# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), r_squared, adj_r_squared, lineal_rmse_train, lineal_rmse_test)
)

# Format the table using kable()
library(knitr)
formatted_table <- kable(results_table, align = "c", col.names = c("Metric", "Value"), format = "pandoc")

# Print the formatted table
formatted_table
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for exercise 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean
robust_lineal_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```

La falta de consideración del "sujeto" en el análisis viola la suposición de independencia, la cual es fundamental en este tipo de modelos. Se asume que todas las observaciones son independientes entre sí, y al no cumplirse esta suposición, se construye un modelo que puede generar métricas incorrectas. Además, al no tener en cuenta la correlación entre los datos, se produce una pérdida de precisión. Los coeficientes estimados pueden presentar un error estándar subestimado, lo que resulta en intervalos de confianza más estrechos.

Otro aspecto a considerar es que la falta de independencia entre las observaciones aumenta el error de Tipo 1, es decir, la probabilidad de rechazar incorrectamente la hipótesis nula y considerar una variable como significativa cuando en realidad no lo es. Esto se debe a que los datos de un mismo individuo tienden a ser más similares, lo que infla la significancia de los resultados.

Por último, cabe destacar que se vuelve más complicado detectar las variaciones dentro de un mismo individuo. En este caso, las muestras deberían ser apareadas, ya que se realiza un seguimiento de 42 individuos a lo largo del tiempo.

En relación al modelo construido, se observa que no es adecuado para predecir la puntuación total de UPDRS. Al analizar los valores de R2 y R2 ajustada, se evidencia que son muy bajos (0.1). Estos coeficientes indican la proporción de variabilidad en la variable dependiente que puede ser explicada por el modelo. Un valor de R2 de 0.1 implica que el modelo apenas puede explicar aproximadamente el 10% de la variabilidad observada en la variable dependiente.

El RMSE (Root Mean Square Error), también conocido como error cuadrático medio, es una medida comúnmente utilizada para evaluar la precisión de un modelo de regresión. El RMSE mide la desviación promedio entre los valores predichos por el modelo y los valores reales de la variable dependiente. Es una medida de la dispersión de los errores, por lo que valores más bajos indican una mayor precisión del modelo, mientras que valores más altos indican una mayor dispersión o un mayor error.

En este caso, el RMSE se sitúa alrededor de 10. Esto significa que, en promedio, las predicciones del modelo presentan un error de aproximadamente 10 unidades en la escala de la variable dependiente. Sin embargo, para interpretar si este valor es considerado bueno o malo, es necesario tener en cuenta la escala y la variabilidad inherente de la variable dependiente.

El UPDRS puede tomar valores en el rango de 0 a 159 según la escala de valoración utilizada (fuente: https://getm.sen.es/profesionales/escalas-de-valoracion/26-getm/escalas-de-valoracion/88-unified-parkinson-s-disease-rating-scale-updrs), aunque en nuestros datos solo se observan valores aproximados entre 7 y 55. Un error de 10 unidades en la escala global no representaría un gran error, ya que equivale a un 6% de la escala completa. Sin embargo, dado que nuestros datos se encuentran en el rango de 7 a 55, un error de 10 unidades representa alrededor de un 17% de la escala disponible, lo cual es significativo.

Es importante destacar una limitación inherente al modelo, y es que al haber sido entrenado con datos de pacientes en estadios tempranos de Parkinson (como se refleja en los valores bajos de UPDRS), este modelo solo sería aplicable para este tipo de pacientes. No se puede garantizar la fiabilidad de los resultados al utilizar el modelo en datos fuera del rango de los datos utilizados para su entrenamiento.

Aunque a primera vista el modelo no parece ser robusto, es importante tener en cuenta que los datos provienen de pacientes con Parkinson. Determinar si un error de predicción del 17% es aceptable o no requiere la opinión de profesionales médicos especializados en el manejo de esta enfermedad. Solo los expertos en Parkinson pueden evaluar la magnitud del error en relación con las implicaciones clínicas y la naturaleza de la enfermedad.

### (b) Regresión lineal con AIC

Utilizando el modelo lineal ajustado previamente en la sección anterior, se aplicó el procedimiento de "stepAIC" para seleccionar de manera automatizada las variables que mejor se ajustan al modelo (Código 11). Al especificar la dirección "both", el procedimiento considerará agregar o eliminar variables en busca de un modelo con un ajuste óptimo. Esta selección se basa en el criterio del valor del AIC, buscando minimizarlo, ya que este valor proporciona una medida del ajuste del modelo teniendo en cuenta su complejidad.

```{r}
# Código 11: Ajuste de regresión lineal por pasos para el ejercicio 2b

library(MASS)

set.seed(123)

# Perform stepwise variable selection based on AIC
model_stepwise <- stepAIC(model_lineal, direction = "both", trace = FALSE)

# Extract R-squared
r_squared = summary(model_stepwise)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_stepwise)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_stepwise, data.train)
# Predict on the test data
predictions_test <- predict(model_stepwise, data.test)

# Calculate residuals
residuals_train <- predictions_train - data.train$total_UPDRS
residuals_test <- predictions_test - data.test$total_UPDRS

# Calculate RMSE train
step_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
step_rmse_test <- sqrt(mean(residuals_test^2))

# Results
# Extract the variable names from the linear regression model
variables <- names(coef(model_stepwise))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), r_squared, adj_r_squared, step_rmse_train, step_rmse_test)
)

# Format the table using kable()
library(knitr)
formatted_table <- kable(results_table, align = "c", col.names = c("Metric", "Value"), format = "pandoc")

# Print the formatted table
formatted_table
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_step_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```

Se observa que, a pesar de la selección paso a paso de variables utilizando el criterio del AIC, el modelo no ha mejorado su ajuste. Aunque ahora se utilicen solo 11 variables en lugar de las 16 originales, lo cual simplifica el modelo, los demás parámetros indican que el modelo aún no es satisfactorio. Como se ha mencionado anteriormente, cuando el modelo presenta valores bajos de R-cuadrado y un alto RMSE, esto indica que el modelo no se ajusta adecuadamente a los datos. Un R-cuadrado bajo implica que el modelo no puede explicar gran parte de la variabilidad en los datos, mientras que un RMSE alto indica que las predicciones del modelo difieren significativamente de los valores reales.

Para mejorar el modelo, se pueden considerar diversas alternativas. Una opción consiste en recopilar más datos con el fin de aumentar el tamaño y la representatividad de la muestra. También se podría explorar la inclusión de variables adicionales que puedan capturar más información relevante. Otra opción es utilizar técnicas de modelado más avanzadas, como regresión no lineal o modelos de aprendizaje automático, que son capaces de capturar relaciones más complejas entre las variables. Estas estrategias podrían contribuir a mejorar el ajuste y la precisión del modelo, permitiendo obtener predicciones más confiables y útiles.

### (c) Regresión por componentes principales

La regresión por componentes principales (PCA, por sus siglas en inglés) es una técnica utilizada para reducir la dimensionalidad de un conjunto de datos. Su objetivo principal es encontrar un conjunto de nuevas variables llamadas componentes principales, que son combinaciones lineales de las variables originales, de modo que retengan la mayor cantidad posible de información en menos dimensiones.

Creamos una regresión por componentes principales usando la función "pcr", especificamos que queremos realizar una validación cruzada. A continuación calculamos los RMSEP sobre cada uno de los modelos (cada modelo tiene un número de componentes distinto, de 0 a 16), con éstos realizamos un gráfico y escogemos el menor valor que sea razonable. Así determinamos el número de componentes óptimos (Código 12). 

En resumen, este código ajusta un modelo de regresión por componentes principales, evalúa su rendimiento mediante la validación cruzada y encuentra el número óptimo de componentes que minimiza el error de predicción.

```{r}
# Código 12: Regresión por componentes principales para el ejercicio 2c

# install.packages('pls')
library(pls)

set.seed(123)

PCA_model <- pcr(total_UPDRS ~ ., data = data.train, validation="CV", scale = TRUE)

# Calculate RMSE values
mpcCV <- RMSEP(PCA_model, estimate = "CV")
rmse_values <- round(mpcCV$val, 1) # Rounded to first decimal to account for complexity

# Plot the graph
plot(mpcCV$comp, rmse_values, type = "b", xlab = "Number of Components", ylab = "RMSE")

# Find the optimal number of components
optimal_components <- mpcCV$comp[which.min(rmse_values)]
points(optimal_components, min(rmse_values), col = "red", pch = 16)
text(optimal_components, min(rmse_values), paste("Optimal:", optimal_components), pos = 3)

optimal_components = optimal_components - 1 # Substract the intercept 
```

En este caso, podemos observar que el número mínimo de componentes es 8, aunque como en los resultados se muestra el intercepto, hay que restarle 1, por lo tanto son 7.

A continuación se calculan las métricas con el número de componentes óptimos (Código 13).

```{r}
# Código 13: Cálculo de métricas para el modelo por PCA, ejercicio 2c

# Predict on the training data using the model
predictions_train <- predict(PCA_model, data.train, ncomp = optimal_components)
# Predict on the test data using the model
predictions_test <- predict(PCA_model, data.test, ncomp = optimal_components)
  
# Calculate residuals for training and test data
residuals_train <- predictions_train - data.train$total_UPDRS
residuals_test <- predictions_test - data.test$total_UPDRS

# Calculate R-squared for training data
SSR <- sum((predictions_train - mean(data.train$total_UPDRS))^2)
SST <- sum((data.train$total_UPDRS - mean(data.train$total_UPDRS))^2)
r2_train <- 1 - SSR/SST

# Calculate adjusted R-squared for training data
n <- nrow(data.train)
p <- length(coef(mr))
r2_adj_train <- 1 - (1 - r2_train) * (n - 1) / (n - p - 1)
  
# Calculate RMSE for training data
PCA_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
PCA_rmse_test <- sqrt(mean(residuals_test^2))

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c((optimal_components), r2_train, r2_adj_train, PCA_rmse_train, PCA_rmse_test)
)

# Format the table using kable()
library(knitr)
formatted_table <- kable(results_table, align = "c", col.names = c("Metric", "Value"), format = "pandoc")

# Print the formatted table
formatted_table

# Robust RMSE for ex3
# Number of observations to trim
n_trim_best <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_best <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_best <- sorted_residuals_best[(n_trim_best + 1):(length(residuals_test) - n_trim_best)]
# Calculate the trimmed mean of the residuals for the best model
trimmed_mean_best <- mean(trimmed_residuals_best)
# Calculate the squared residuals using the trimmed mean for the best model
trimmed_squared_residuals_best <- (trimmed_residuals_best - trimmed_mean_best)^2
# Calculate the robust RMSE using the trimmed mean for the best model
robust_PCA_rmse_test <- sqrt(mean(trimmed_squared_residuals_best))
```

Basado en los resultados obtenidos, se puede concluir que el modelo ajustado utilizando PCA no logra un buen ajuste ni una buena capacidad predictiva para el conjunto de datos utilizado. Esto se refleja en los valores bajos de R2 y los altos valores de RMSE. Estos indicadores sugieren que el modelo no captura de manera adecuada la variabilidad de la variable objetivo y presenta un nivel significativo de error en las predicciones. Por lo tanto, se recomienda explorar otras técnicas de modelado y considerar diferentes variables o enfoques para obtener un mejor rendimiento y una mayor precisión en las predicciones.

### (d) Ridge regression

La regresión de Ridge es un método de regresión regularizada que se utiliza para abordar el problema de multicolinealidad en modelos de regresión lineal. La multicolinealidad se produce cuando hay una alta correlación entre las variables predictoras, lo que puede afectar negativamente la precisión de los coeficientes estimados.

En la regresión de Ridge, se introduce una penalización de L2 a la función objetivo, que se suma a la suma de los cuadrados de los errores. Esta penalización restringe los coeficientes estimados, haciendo que sean más pequeños en magnitud. Esto ayuda a reducir la influencia de las variables predictoras altamente correlacionadas y evita la amplificación de errores debido a la multicolinealidad.

En el siguiente código se determina el mejor valor de lambda de 0 a 0.1 mediante validación cruzada, esta es una técnica común en estadística. Una vez determinado el mejor valor de lambda se ajusta nuevamente una regresión Ridge con este valor (Código 14).

```{r}
# Código 14: Ajuste de un modelo de regresión Ridge, ejercicio 2d

library(MASS)

set.seed(123)

# Perform Ridge Regression with a range of lambda values
mr <- lm.ridge(total_UPDRS ~ ., data = data.train, lambda = seq(0, 0.1, 0.001))

# Find the index of the lambda value that minimizes the GCV score
nGCV <- which.min(mr$GCV)

# Retrieve the lambda value that minimizes the GCV score
lGCV <- mr$lambda[nGCV]

# Visualize the GCV scores as a function of lambda
plot(mr$lambda, mr$GCV, type = "l", xlab = expression(lambda), ylab = "GCV")

# Add a vertical line at the lambda value that minimizes the GCV score
abline(v = lGCV, col = 2)

# Print the lambda value that minimizes the GCV score
cat("Mejor lambda: ", lGCV)

# Refit the Ridge Regression model using the optimal lambda value
mr <- lm.ridge(total_UPDRS ~ ., data = data.train, lambda = lGCV)

# Make predictions (no y this time)
predictions_test = cbind(1,as.matrix(data.test[,-1])) %*% coef(mr)
predictions_train = cbind(1,as.matrix(data.train[,-1])) %*% coef(mr)

# Calculate residuals for training and test data
residuals_train <- predictions_train - data.train$total_UPDRS
residuals_test <- predictions_test - data.test$total_UPDRS

# Calculate R-squared for training data
r2_train <- 1 - sum(residuals_train^2) / sum((data.train$total_UPDRS - mean(data.train$total_UPDRS))^2)

# Calculate adjusted R-squared for training data
n_train <- nrow(data.train)
p_train <- optimal_components - 1  # Number of predictors (components) used
r2_adj_train <- 1 - (1 - r2_train) * ((n_train - 1) / (n_train - p_train - 1))
  
# Calculate RMSE for training data
ridge_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
ridge_rmse_test <- sqrt(mean(residuals_test^2))

# Extract the variable names
variables <- names(coef(mr))

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c((length(variables) -1), r2_train, r2_adj_train, ridge_rmse_train, ridge_rmse_test)
)

# Format the table using kable()
library(knitr)
formatted_table <- kable(results_table, align = "c", col.names = c("Metric", "Value"), format = "pandoc")

# Print the formatted table
formatted_table
cat("\n Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for ex 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_ridge_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```

Basado en los resultados obtenidos, se puede concluir que el modelo ajustado utilizando Ridge Regression no logra un buen ajuste y presenta un rendimiento predictivo deficiente para el conjunto de datos utilizado. Esto se evidencia por los valores bajos de R2 y los altos valores de RMSE.

En conjunto, estos resultados sugieren que el modelo de Ridge Regression no es capaz de capturar la relación adecuada entre las variables predictoras y la variable objetivo en el conjunto de datos utilizado. Es posible que se requieran ajustes adicionales, como la selección de variables más relevantes, la exploración de diferentes transformaciones de variables o la consideración de otros modelos más adecuados para mejorar el rendimiento y la precisión de las predicciones.

### (e) motor_UPDRS como respuesta

Dada la falta de ajuste y la baja capacidad predictiva de los modelos anteriores, incluyendo el modelo de Regresión Ridge, sí es conveniente considerar la posibilidad de ajustar el modelo utilizando la variable de respuesta alternativa disponible, "motor_UPDRS".

La inclusión de la variable "motor_UPDRS" como nueva variable de respuesta permitirá investigar si el modelo puede lograr un mejor ajuste y una mayor precisión predictiva en comparación con los modelos anteriores. Esto resulta especialmente relevante si existe una relación más fuerte o significativa entre las variables predictoras (medidas de voz biométricas) y la variable "motor_UPDRS".

En resumen, dada la falta de ajuste y la baja capacidad predictiva de los modelos anteriores, es recomendable explorar la opción de ajustar el modelo utilizando la variable de respuesta "motor_UPDRS". Esto permitirá evaluar si esta variable puede ser mejor pronosticada por las variables predictoras disponibles. Si bien esto no cumpliría el objetivo principal de los modelos de regresión ajustados previamente, que era predecir el UPDRS total utilizando las 16 medidas de voz, vale la pena intentarlo con la variable "motor_UPDRS" para obtener información relevante sobre su capacidad de predicción.

### (f) Análisis de residuos

Las principales suposiciones del modelo de regresión de Mínimos Cuadrados Ordinarios (OLS) son las siguientes: linealidad, independencia, homocedasticidad, ausencia de multicolinealidad y normalidad.

Con el fin de evaluar la adecuación de los datos para un modelo lineal, se realizaron los siguientes gráficos (Código 15):

  - Residuals vs. Fitted (which = 1): Se busca examinar la **linealidad**, observando si los valores residuales se distribuyen alrededor de cero. En general, se cumple la suposición de linealidad, aunque se observan algunos valores extremos que se alejan de cero.
  
  - Normal Q-Q plot (which = 2): Permite evaluar la **normalidad**, verificando si los valores se alinean con la recta. Aunque no se ajustan perfectamente a la recta de referencia, en general se puede decir que se cumple la suposición de normalidad.
  
  - Scale-Location plot (which = 3): Sirve para evaluar la **homocedasticidad**, buscando que los datos se distribuyan de manera equitativa sin que la dispersión varíe. Al no observarse una forma de cono en los datos, se puede concluir que se cumple la suposición de homocedasticidad.
  
  - Cook's distance plot (which = 4): Se utiliza para identificar puntos influyentes. Se observa que hay tres puntos con valores de distancia de Cook superiores al resto. Sin embargo, ninguno de ellos presenta valores mayores a 1, que es el umbral comúnmente utilizado para determinar la influencia de los puntos.
  
  - Residuals vs. Leverage plot (which = 5): ambién se utiliza para identificar puntos influyentes, y los mismos tres puntos mencionados anteriormente son marcados como los más influyentes.

```{r}
# Código 15: Análisis de asunciones, ejercicio 2f

# Residual plot
plot(model_lineal, which = 1)

# Normal Q-Q plot
plot(model_lineal, which = 2)

# Scale-location plot (square root of standardized residuals)
plot(model_lineal, which = 3)

# Cook's distance
plot(model_lineal, which = 4)

# Residuals vs. fitted values plot
plot(model_lineal, which = 5)
```

Para evaluar la presencia de multicolinealidad, se realizó el cálculo de una matriz de correlación, así como la obtención de los valores de VIF (Factor de Inflación de la Varianza) y los valores de tolerancia (Código 16).

```{r}
# Código 16: Estudio de correlación, ejercicio 2f

cor_matrix <- cor(data.train[, c("Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")])

# install.packages("corrplot")
library(corrplot)
corrplot(cor_matrix, method = "color")

library(car)
vif_values <- vif(model_lineal)

tolerance_values <- 1/vif_values

# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_cor <- data.frame(
  VIF = vif_values,
  Tolerance = tolerance_values
)

# Format the table using kable()
library(knitr)
formatted_table <- kable(results_cor, align = "c", col.names = c("VIF", "Tolerancia"), format = "pandoc")

# Print the formatted table
formatted_table
```

El análisis de la matriz de correlación reveló que la mayoría de los tipos de Jitter y Shimmer muestran una correlación significativa entre ellos. Además, se observó una correlación negativa entre el HNR y Shimmer.

El valor de VIF proporciona información sobre el grado en que la varianza de un coeficiente de regresión se ve inflada debido a la correlación lineal con otras variables independientes. En general, se considera que existe una correlación problemática entre las variables independientes cuando el valor del VIF es mayor a 1, y especialmente cuando supera un umbral de 5 o 10. Por otro lado, la tolerancia se calcula como "Tolerancia = 1 / VIF" y varía entre 0 y 1. Un valor cercano a 1 indica poca multicolinealidad, mientras que un valor cercano a 0 indica una alta multicolinealidad.

En este caso, se observaron valores extremadamente altos de VIF, lo que indica una grave presencia de multicolinealidad entre las variables. Asimismo, se encontraron valores muy bajos de tolerancia para la mayoría de las variables.

Adicionalmente, se examinó la correlación entre los residuos consecutivos (Código 17). Aunque no se consideró en el presente análisis, es importante mencionar que los datos corresponden a mediciones en el tiempo de los mismos 42 individuos. El gráfico mostrado a continuación nos ayuda a identificar la presencia de autocorrelación o correlación temporal.

```{r}
# Código 17: Correlación entre residuos, ejercicio 2f

# Calculate residuals
residuals <- residuals(model_lineal)

# Plot residuals
plot(residuals[-length(residuals)], residuals[-1], xlab = "Residual i", ylab = "Residual i+1", main = "Correlation between residuals")
```

En el gráfico presentado, se observa una clara relación lineal positiva, lo que indica que los residuos en un momento dado están correlacionados positivamente con los residuos en el siguiente momento. En otras palabras, si un residuo es mayor de lo esperado, es probable que el residuo siguiente también sea mayor de lo esperado, y si un residuo es menor de lo esperado, el residuo siguiente tiende a ser menor de lo esperado.

Esta correlación positiva entre los residuos consecutivos señala la presencia de autocorrelación en los residuos del modelo de regresión.

En vista de estos resultados, resultaría interesante considerar la inclusión del factor sujeto en el modelo, ya que podría ser la causa del bajo valor predictivo observado. Además, la incorporación de variables adicionales con una menor correlación entre sí podría ser beneficioso. Sería oportuno explorar la inclusión de diferentes tipos de mediciones de voz u otros parámetros que puedan contribuir a la predicción más precisa de la puntuación total de UPDRS.

# Ejercicio 3

### (a) Comparación modelos con y sin puntos influyentes

```{r}
# Calculate cooks distance
cooksd <- cooks.distance(model_lineal)

# Get 3 most influential points
top3_indices <- order(cooksd, decreasing = TRUE)[1:3]

# Remove the points
data.train <- data.train[-top3_indices, ]
```

Re-execute everything but with clean data

```{r}
# Fit the model
model_lineal = lm(total_UPDRS ~ ., data = data.train)

# Extract R-squared
r_squared = summary(model_lineal)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_lineal)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_lineal, data.train)
# Predict on the test data
predictions_test <- predict(model_lineal, data.test)

# Calculate residuals train
residuals_train = data.train$total_UPDRS - predictions_train
# Calculate residuals test
residuals_test <- data.test$total_UPDRS - predictions_test

# Calculate RMSE train
lineal_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
lineal_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Create results table
# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c(length(variables), r_squared, adj_r_squared, lineal_rmse_train, lineal_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for exercise 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean
robust_lineal_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```
________________________________________
```{r}
library(MASS)

set.seed(123)

# Perform stepwise variable selection based on AIC
model_stepwise <- stepAIC(model_lineal, direction = "both", trace = FALSE)

# Extract R-squared
r_squared = summary(model_stepwise)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_stepwise)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_stepwise, data.train)
# Predict on the test data
predictions_test <- predict(model_stepwise, data.test)

# Calculate residuals
residuals_train = data.train$total_UPDRS - predictions_train
# Calculate residuals
residuals_test <- data.test$total_UPDRS - predictions_test

# Calculate RMSE train
step_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
step_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Results
# Extract the variable names from the linear regression model
variables <- names(coef(model_stepwise))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c(length(variables), r_squared, adj_r_squared, step_rmse_train, step_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_step_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))

mean(residuals_test, trim=0.1)
```
______________________________________________________________________
```{r}
# install.packages('pls')
library(pls)

set.seed(123)

PCA_model <- pcr(total_UPDRS ~ ., data = data.train, validation="CV", scale = TRUE)

# Calculate RMSE values
mpcCV <- RMSEP(PCA_model, estimate = "CV")
rmse_values <- round(mpcCV$val, 1) # Rounded to first decimal to account for complexity

# Plot the graph
plot(mpcCV$comp, rmse_values, type = "b", xlab = "Number of Components", ylab = "RMSE")

# Find the optimal number of components
optimal_components <- mpcCV$comp[which.min(rmse_values)]
points(optimal_components, min(rmse_values), col = "red", pch = 16)
text(optimal_components, min(rmse_values), paste("Optimal:", optimal_components), pos = 3)
```
```{r}
# Predict on the training data using the model
predictions_train <- predict(PCA_model, data.train, ncomp = optimal_components-1)
# Predict on the test data using the model
predictions_test <- predict(PCA_model, data.test, ncomp = optimal_components-1)
  
# Calculate residuals for training and test data
residuals_train <- data.train$total_UPDRS - predictions_train
residuals_test <- data.test$total_UPDRS - predictions_test

# Calculate R-squared for training data
r2_train <- 1 - sum(residuals_train^2) / sum((data.train$total_UPDRS - mean(data.train$total_UPDRS))^2)

# Calculate adjusted R-squared for training data
n_train <- nrow(data.train)
p_train <- optimal_components - 1  # Number of predictors (components) used
r2_adj_train <- 1 - (1 - r2_train) * ((n_train - 1) / (n_train - p_train - 1))
  
# Calculate RMSE for training data
PCA_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
PCA_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c((optimal_components - 1), r2_train, r2_adj_train, PCA_rmse_train, PCA_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for ex3
# Number of observations to trim
n_trim_best <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_best <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_best <- sorted_residuals_best[(n_trim_best + 1):(length(residuals_test) - n_trim_best)]
# Calculate the trimmed mean of the residuals for the best model
trimmed_mean_best <- mean(trimmed_residuals_best)
# Calculate the squared residuals using the trimmed mean for the best model
trimmed_squared_residuals_best <- (trimmed_residuals_best - trimmed_mean_best)^2
# Calculate the robust RMSE using the trimmed mean for the best model
robust_PCA_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_best))
```
______________________________________________________________________
```{r}
library(MASS)

set.seed(123)

mr <- lm.ridge(total_UPDRS ~ ., data = data.train, lambda=(seq(0, 0.1, 0.001)))
(nGCV <- which.min(mr$GCV))

lGCV <- mr$lambda[nGCV]
matplot(mr$lambda,coef(mr),type="l", ylim=c(-2,2), xlab=expression(lambda),ylab=expression(hat(beta[i])))
abline(v=lGCV,col=2)

plot(mr$lambda,mr$GCV,type="l",xlab=expression(lambda),ylab="GCV")
abline(v=lGCV,col=2)

mr <- lm.ridge(total_UPDRS ~ ., data = data.train, lambda=lGCV)
```
```{r}
# Make predictions (no y this time)
predictions_test = cbind(1,as.matrix(data.test[,-1])) %*% coef(mr)
predictions_train = cbind(1,as.matrix(data.train[,-1])) %*% coef(mr)

# Calculate residuals for training and test data
residuals_train <- data.train$total_UPDRS - predictions_train
residuals_test <- data.test$total_UPDRS - predictions_test

# Calculate R-squared for training data
r2_train <- 1 - sum(residuals_train^2) / sum((data.train$total_UPDRS - mean(data.train$total_UPDRS))^2)

# Calculate adjusted R-squared for training data
n_train <- nrow(data.train)
p_train <- optimal_components - 1  # Number of predictors (components) used
r2_adj_train <- 1 - (1 - r2_train) * ((n_train - 1) / (n_train - p_train - 1))
  
# Calculate RMSE for training data
ridge_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
ridge_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Extract the variable names
variables <- colnames(data.test[,-1])

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c((optimal_components - 1), r2_train, r2_adj_train, ridge_rmse_train, ridge_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for ex 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_ridge_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```

```{r}
# Create a data frame for the results
results <- data.frame(
  Modelo = c("OLS", "AIC", "RCP", "Ridge"),
  RMSE_Con = c(lineal_rmse_test, step_rmse_test, PCA_rmse_test, ridge_rmse_test),
  RMSE_Sin = c(lineal_clean_rmse_test, step_clean_rmse_test, PCA_clean_rmse_test, ridge_clean_rmse_test)
)

print(results)
```

```{r}
# Checking if the points were correctly deleted
# Re-calculate cooks distance
cooksd_updated <- cooks.distance(model_lineal) # Clean data

# Plot both
plot(cooksd_updated, pch = 20, cex = 1.5, main = "Cook's Distance Plot Updated")
plot(cooksd, pch = 20, cex = 1.5, main = "Cook's Distance Plot")
```

### (b) Cálculo del RMSE robusto

The trimmed mean is less sensitive to outliers and can provide a more robust estimate of the central tendency.

```{r}
# Create a data frame for the results
results2 <- data.frame(
  Modelo = c("OLS", "AIC", "RCP", "Ridge"),
  RMSE_Con = c(lineal_rmse_test, step_rmse_test, PCA_rmse_test, ridge_rmse_test),
  RMSE_Sin = c(lineal_clean_rmse_test, step_clean_rmse_test, PCA_clean_rmse_test, ridge_clean_rmse_test),
  Robusto_Con = c(robust_lineal_rmse_test, robust_step_rmse_test, robust_PCA_rmse_test, robust_ridge_rmse_test),
  Robusto_Sin = c(robust_lineal_clean_rmse_test, robust_step_clean_rmse_test, robust_PCA_clean_rmse_test, robust_ridge_clean_rmse_test)
)

print(results2)
```

### (c) LTS o Huber

```{r}
set.seed(123)

# Data import
# Note: I changed the variable names to avoid problems with symbols

import.data <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data"
parkinson <- read.table(url(import.data), sep=",", skip=1)
names(parkinson) <- c("subject#","age","sex","test_time","motor_UPDRS","total_UPDRS",
"Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")

# Select predictor variables and response
library(dplyr)
set.seed(123)
parkinson <- parkinson %>% dplyr::select(6:22)

# Split the data into train and test
library(caret)
set.seed(123)
train_indices <- createDataPartition(parkinson$total_UPDRS, p = 0.8, list = FALSE)
data.train <- parkinson[train_indices, ]
data.test <- parkinson[-train_indices, ]

# Huber
require(MASS)
hub <- rlm(total_UPDRS ~ ., data = data.train)
summary(hub)

# Predict on train and test data
train_pred <- predict(hub, newdata = data.train)
test_pred <- predict(hub, newdata = data.test)

# Calculate R-squared
train_r2 <- 1 - sum((data.train$total_UPDRS - train_pred)^2) / sum((data.train$total_UPDRS - mean(data.train$total_UPDRS))^2)
test_r2 <- 1 - sum((data.test$total_UPDRS - test_pred)^2) / sum((data.test$total_UPDRS - mean(data.train$total_UPDRS))^2)

# Calculate adjusted R-squared
n <- nrow(data.train)
p <- length(coef(hub))
train_r2_adj <- 1 - (1 - train_r2) * ((n - 1) / (n - p - 1))
test_r2_adj <- 1 - (1 - test_r2) * ((n - 1) / (n - p - 1))

# Calculate RMSE
train_rmse <- sqrt(mean((data.train$total_UPDRS - train_pred)^2))
test_rmse <- sqrt(mean((data.test$total_UPDRS - test_pred)^2))

# Create results table
# Extract the variable names from the linear regression model
variables <- names(coef(hub))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), train_r2, train_r2_adj, train_rmse, test_rmse)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")
```

# ANEXO

### Código