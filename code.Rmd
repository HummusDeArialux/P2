---
title: "PAC 2 Regresión Lineal"
author: "Maria Lucas"
date: "2023-06-22"
output: 
  word_document:
    toc: true
    toc_depth: 4
lang: Es-es
---
\newpage

# Ejercicio 1

```{r}
# Set viasulization number
options(scipen = 999)

# Set the file path
file_path <- "D:/Antiguos estudios/MASTER2/Sem2/Regresion/PAC2/P2/pancreas_biomarkers.txt"

# Load the data into a data frame
data <- read.table(file_path, header = TRUE, sep = "\t")

# Display the first few rows of the data frame
head(data)

# Saving variables as factors
data$diagnosis <- factor(data$diagnosis, levels = 1:3)
data$age_cat <- factor(data$age_cat)
```

### (a) Modelo de regresión logística

#### Diagnóstico de todos los casos
```{r}
model <- glm(diagnosis ~ age_cat + creatinine + LYVE1 + REG1B + TFF1, data = data, family = binomial)

summary(model)
```

Tal y como podemos ver en los resultados, tan solo la creatinina, la LYVE1 y el TFF1 son variables que permiten predecir el riesgo de adenocarcinoma ductal pancreático. Se puede observar porque en todas ellas el pvalor es menos a 0.01. En el contexto de regresión logística un pvalor de 0.01 permite rechazar la hipótesis nula de no relación entre la variable predictora y la variable respuesta. En otras palabras, la variable tiene un impacto significativo sobre la clase.

#### Sólo adenocarcinoma y otro

```{r}
# Subset the data to include only levels 2 and 3 of the "diagnosis" variable
subset_data <- subset(data, diagnosis != 1)

# Recode values 2 and 3 to 0 and 1, respectively
subset_data$diagnosis <- ifelse(subset_data$diagnosis == 2, 0, 1)

# Fit the logistic regression model using the subsetted data
model <- glm(diagnosis ~ age_cat + creatinine + LYVE1 + REG1B + TFF1, data = subset_data, family = binomial)

# View the model summary
summary(model)
```

Tal y como podemos ver en los resultados, la edad a partir de 56 años es un indicador significativo. La LYVE1 y el REG1B también son variables que permiten predecir el riesgo de adenocarcinoma ductal pancreático. Se puede observar porque en todas ellas el pvalor es menos a 0.05. En el contexto de regresión logística un pvalor de 0.01 permite rechazar la hipótesis nula de no relación entre la variable predictora y la variable respuesta. En otras palabras, la variable tiene un impacto significativo sobre la clase.

Que el intercepto también sea significativo sugiere que parte de la variable respuesta no es explicada por las variables independientes estudiadas. En regresión logística el intercepto captura la probabilidad de ocurrencia de un suceso cuando todas las variables predictoras están en su nivel de referencia. En otras palabras, un intercepto significativo implica que aunque no tengamos ningun predictor, hay diferencias significativas entre las clases.

### (b) Interpretación de coeficientes

Los coeficientes en un modelo de regresión logística indican el cambio estimado en el log-odds del evento ocurriendo (codeado como 1 = adenocarcinoma ductal pancreático) asociado a una unidad de cambio en la variable predictora, sin variar el resto de variables.

Un estimador positivo sugiere que el incremento de la variable está asociado a un incremento de la probabilidad (log-odds) del evento ocurriendo. Si el estimador es 0.5 significa que al aumentar en 1 el valor del predictor, esto se asopcia a un 0.5 aumento del log-odds del evento ocurriendo. Este es el caso del LYVE1, al aumentar LYVE1 es más probable tener adenocarcinoma (0.3140). Lo mismo sucede con la edad, parece ser que tener más de 56 años aumenta el log-odds de tener adenocacinoma. Lo hace de forma distinta dependiendo del rango de edad, de 56-65 (3.032), de 66-75 (2.7) y a partir de 75 años (3.443).

Contrariamente, un estimador negativo significa que un aumento de la variable disminuye el log-odds del evento ocurriendo (tener adenocarcinoma).

### (c) Modelo reducido

Para comparar ambos modelos podemos realizar un anova aplicando el test Chi cuadrado. Con esto estamos comparando el ajuste del modelo bajo las siguientes hipótesis:

- **H0:** Desviación del modelo reducido = Desviación del modelo completo. No hay diferencia de ajuste entre los modelos.
- **H1:** Desviación del modelo reducido > Desviación del modelo completo. El modelo reducido se ajusta peor que el modelo completo.

La desviación del modelo se refiere a la diferencia entre los valores reales de los datos y los valores predichos por el modelo.

```{r}
# Fit the logistic regression model using the subsetted data
model_simple <- glm(diagnosis ~ age_cat + LYVE1 + REG1B, data = subset_data, family = binomial)

summary(model_simple)

# Compare the two models
anova(model, model_simple, test = "Chi")

# Install and load the "lmtest" package
# install.packages("lmtest")
library(lmtest)

# Perform the likelihood ratio test
lrtest(model, model_simple)

```

Dado que el pvalor no es significativo (0.31) aceptamos la hipótesis nula, el modelo reducido (sin creatinina y sin TFF1) es igual de bueno que el complejo. Adicionalmente, vemos que el AIC del modelo reducido es 2 unidades menor que el del modelo complejo, esto sugiere que el modelo reducido tiene un ajuste mejor teniendo en cuenta la complejidad de ambos modelos.

### (d) Funcion cuadrática

```{r}
# Cuadratic LYVE1
model_simple_LYVE1 = glm(diagnosis ~ age_cat + LYVE1 + I(LYVE1^2) + REG1B, data = subset_data, family = binomial)

summary(model_simple_LYVE1)

# Compare the two models
anova(model_simple, model_simple_LYVE1, test = "Chi")

# Cuadratic REG1B
model_simple_REG1B = glm(diagnosis ~ age_cat + LYVE1 + REG1B + I(REG1B^2), data = subset_data, family = binomial)

summary(model_simple_REG1B)

# Compare the two models
anova(model_simple, model_simple_REG1B, test = "Chi")

```

En ninguno de los dos casos añadir el término cuadrático ha mejorado el ajuste del modelo. Siguiendo la explicación del apartado anterior: pv = 0.3 y 0.4, aceptamos H0 los dos modelos tienen el mismo ajuste.

Adicionalmente, si examinamos el AIC veremos que en los modelos con el término cuadrático este es superior. Un valor mayor AIC sugiere que el modelo no se ajusta mejor para la complejidad que presenta.

En conclusión, no deberíamos incluir ninguno de los dos términos cuadráticos. Es importante comentar que, aunque estos tests aparezcan no-significativos, hay que valorarlos siempre junto con el contexto del análisis. En este caso, se ha realizado adicionalmente una inspección visual de la variable vs el log odds de la variable respuesta (disponible en el ANEXO). Estos gráficos nos confirman que efectivamente, existe una relación lineal, dado que se observa un patrón lineal sin observar otros patrones como curvas, forma de U, etc.

```{r}
# Obtain predicted log odds from the model
predicted_logodds <- predict(model_simple, type = "link")

plot(subset_data$LYVE1, predicted_logodds, xlab = "age_cat", ylab = "Log Odds", main = "Scatter plot - LYVE1 vs. Log Odds")

plot(subset_data$REG1B, predicted_logodds, xlab = "age_cat", ylab = "Log Odds", main = "Scatter plot - REG1B vs. Log Odds")
```

### (e) Predicción de caso

```{r}
# Create a new data frame for the new case
new_case = data.frame(age_cat = "66-75", LYVE1 = 6, REG1B = 140)

# Predict the outcome using the model
prediction = predict(model_simple, newdata = new_case, type = "response")

print(prediction)
```
El modelo predice la presencia de adenocarcinoma ductal pancreático con un 72,42% de probabilidad. Recordemos que los valores fueron recodificados a 0 = afecciones pancreáticas no cancerosas y 1 = adenocarcinoma ductal pancreático.

La extrapolación ocurre cuando se hacen predicciones de para datos de la variable predictora fuera del rango de los datos usados para contruir el modelo.

```{r}
# Check predictor variable ranges
range_data <- sapply(subset_data[, c("LYVE1", "REG1B")], range)
range_new_observation <- c(6, 140)  # Replace with the values of the new observation

# Compare new observation values with observed range
is_extrapolation <- any(range_new_observation < range_data[1, ] | range_new_observation > range_data[2, ])

# Print results
cat("Extrapolation:", is_extrapolation, "\n")

# Assess distribution of predictor variables
# LYVE1
hist(subset_data$LYVE1, main = "Distribution of LYVE1")
# Add new observation to LYVE1 plot
points(6, 0, col = "red", pch = 16)

# REGB1
hist(subset_data$REG1B, main = "Distribution of REG1B")
# Add new observation to REGB1 plot
points(140, 0, col = "red", pch = 16)

# age_cat
barplot(table(subset_data$age_cat), main = "Distribution of age_cat", xlab = "age_cat", ylab = "Frequency")
# Add new observation to age_cat plot
points(5.5, 0, col = "red", pch = 16)
```

Como podemos ver, los valores del caso a predecir se encuentra entre el rango utilizado para construir el modelo.

# Ejercicio 2

```{r}
set.seed(123)

# Data import
# Note: I changed the variable names to avoid problems with symbols

import.data <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data"
parkinson <- read.table(url(import.data), sep=",", skip=1)
names(parkinson) <- c("subject#","age","sex","test_time","motor_UPDRS","total_UPDRS",
"Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")

# install.packages('caTools')
library(caTools)

# Split the data into training and testing sets
split = sample.split(parkinson, SplitRatio = 0.8)
train_data = parkinson[split, ]
test_data = parkinson[!split, ]
```

### (a) Regresión lineal

Al no utilizar el factor "sujeto" no tenemos en cuenta las posibles variaciones de cada individuo. Las muestras deberían ser apareadas, ya que se ace un seguimiento a lo largo del tiempo. Una estimación. No sé lo que digo ya lo mirare.

```{r}
# Fit the model
model_lineal = lm(total_UPDRS ~ Jitter_p + Jitter_Abs + Jitter_RAP + Jitter_PPQ5 + Jitter_DDP + Shimmer + Shimmer_dB + Shimmer_APQ3 + Shimmer_APQ5 + Shimmer_APQ11 + Shimmer_DDA + NHR + HNR + RPDE + DFA + PPE, data = train_data)

# Extract R-squared
r_squared = summary(model_lineal)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_lineal)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_lineal, train_data)

# Calculate residuals
residuals_train = train_data$total_UPDRS - predictions_train

# Calculate RMSE
rmse_train = sqrt(mean(residuals_train^2))

# Predict on the test data
predictions_test <- predict(model_lineal, test_data)

# Calculate residuals
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate RMSE
rmse_test_lineal <- sqrt(mean(residuals_test^2))

# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))

# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)

# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]

# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)

# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2

# Calculate the robust RMSE using the trimmed mean:
robust_rmse_lineal <- sqrt(mean(trimmed_squared_residuals_test))

# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), r_squared, adj_r_squared, rmse_train, rmse_test_lineal)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")
```

Al no considerar el "sujeto" se viola la suposición de independencia. En este tipo de modelos, se asume que todas las observaciones son independientes. Al no serlo, se construirá un modelo que incluirá métricas erróneas. De manera similar, se pierde precisión al no tener en cuenta la correlación entre los datos. El error estándar de los coeficientes estimados puede subestimarse, dando intervalos de confianza más estrechos.

Adicionalmente, se aumenta el error de Tipo 1 (rechazar incorrectamente H0 haciendo que una variable sea significativa cuando no lo es). Esto sucede porque los datos de un mismo individuo tienden a ser más similares, inflando así la significación de los resultados. 

Finalmente, se hace más complicado detectar las variaciones entre en un mismo individuo. 

### (b) Regresión lineal con AIC

```{r}
library(MASS)

# Perform stepwise variable selection based on AIC
model_stepwise <- stepAIC(model_lineal, direction = "both", trace = FALSE)

# Extract R-squared
r_squared = summary(model_stepwise)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_stepwise)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_stepwise, train_data)

# Calculate residuals
residuals_train = train_data$total_UPDRS - predictions_train

# Calculate RMSE
rmse_train = sqrt(mean(residuals_train^2))

# Predict on the test data
predictions_test <- predict(model_stepwise, test_data)

# Calculate residuals
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate RMSE
rmse_test_AIC <- sqrt(mean(residuals_test^2))

# Extract the variable names from the linear regression model
variables <- names(coef(model_stepwise))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), r_squared, adj_r_squared, rmse_train, rmse_test_AIC)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")
```
Removing a variable solely based on its lack of significance may affect the overall fit of the model and the relationships between other variables. A variable that is not individually significant may contribute to the model's overall predictive power when combined with other variables or interacted with other predictors. Therefore, it's crucial to assess the model's overall performance, such as through measures like R-squared or adjusted R-squared, and consider the context and theoretical significance of the variables

The significance of a variable can be influenced by multicollinearity, which occurs when predictor variables are highly correlated with each other. In the presence of multicollinearity, the individual coefficients and their significance can be unstable or misleading. It's important to check for multicollinearity among the variables and consider its potential impact on the significance of individual predictors.

In this example, model_linear is the initial linear regression model you built using all the variables of interest. The stepAIC() function from the MASS package is used to perform the stepwise variable selection based on AIC.

The direction argument specifies the direction of the stepwise procedure. "both" allows variables to be added or removed from the model. Other options include "backward" for variable removal only and "forward" for variable addition only.

### (c) Regresión por componentes principales

Performing regression using principal components involves transforming the predictor variables into a set of principal components and then using these components as predictors in the regression model.

```{r}
# Select predictor variables
predictor_vars = train_data[7:22]

# Initialize variables for best model
best_rmse <- Inf
best_components <- 0
best_model <- NULL

# Perform principal component analysis (PCA) on the predictor variables
pca <- prcomp(predictor_vars, scale = TRUE)

# Extract the principal components
pcs <- pca$x

# Identify the number of components with minimum RMSE
num_components <- 1:ncol(pcs)
rmse_values <- numeric(length(num_components))

for (i in num_components) {
  # Select the first i principal components
  pcs_selected <- pcs[, 1:i]
  
  # Create a regression model using the selected components
  model_pca <- lm(total_UPDRS ~ pcs_selected, data = train_data)
  
  # Predict on the test data using the model
  predictions <- predict(model_pca, test_data)
  
  # Calculate residuals
  residuals <- test_data$total_UPDRS - predictions
  
  # Calculate RMSE
  rmse_values[i] <- sqrt(mean(residuals^2))
  
  # Check if current model has lower RMSE
  if (rmse_values[i] < best_rmse) {
    best_rmse <- rmse_values[i]
    best_components <- i
    best_model <- model_pca
  }
}

# Plot the RMSE values against the number of components
plot(num_components, rmse_values, type = "b", xlab = "Number of Components", ylab = "RMSE")

# Identify the number of components with the minimum RMSE
min_rmse <- min(rmse_values)
optimal_components <- num_components[which.min(rmse_values)]

# Print the results
print(paste("Minimum RMSE:", min_rmse))
print(paste("Optimal Number of Components:", optimal_components))
```
The code performs principal component analysis (PCA) on the predictor variables using the prcomp() function. The resulting principal components (pcs) are then used to construct regression models with different numbers of components (ranging from 1 to the total number of components).

For each model, the code predicts the outcome variable on the test data, calculates the residuals, and computes the root mean squared error (RMSE). The RMSE values are stored in the rmse_values vector.

The code then plots the RMSE values against the number of components to visualize the relationship. The number of components that yields the minimum RMSE is identified, and the corresponding results are printed.

Yes, in general, a lower RMSE indicates a better-fitting model. RMSE (Root Mean Squared Error) is a commonly used measure of the average prediction error of a regression model. It represents the square root of the average squared differences between the predicted values and the actual values of the outcome variable.

Since RMSE measures the magnitude of the prediction errors, a smaller RMSE implies that the model's predictions are, on average, closer to the actual values. 

```{r}
# Predict on the training data using the model
predictions_train <- predict(model_pca, train_data)
  
# Predict on the test data using the model
predictions_test <- predict(model_pca, test_data)
  
# Calculate residuals for training and test data
residuals_train <- train_data$total_UPDRS - predictions_train
residuals_test <- test_data$total_UPDRS - predictions_test
  
# Calculate R-squared for training data
R_squared_train <- summary(model_pca)$r.squared
  
# Calculate adjusted R-squared for training data
num_predictors <- length(coefficients(model_pca)) - 1
n <- nrow(train_data)
adjusted_R_squared_train <- 1 - (1 - R_squared_train) * ((n - 1) / (n - num_predictors - 1))
  
# Calculate RMSE for training data
RMSE_train <- sqrt(mean(residuals_train^2))
  
# Calculate RMSE for test data
RMSE_test <- sqrt(mean(residuals_test^2))

# Extract the variable names from the linear regression model
variables <- names(coef(best_model))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), R_squared_train, adjusted_R_squared_train, RMSE_train, RMSE_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")
```

Nota: el mejor modelo según el RMSE test no el train.

### (d) Ridge regression

Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.

Yes, you can adjust the model using Ridge regression. Ridge regression is a regularization technique that introduces a penalty term to the least squares objective function, helping to reduce the impact of multicollinearity and potentially improve the model's performance.

```{r}
# install.packages('glmnet')
library(glmnet)

# Select predictor variables as matrix
x_train = data.matrix(train_data[7:22])
y_train = train_data$total_UPDRS

x_test = data.matrix(test_data[7:22])
y_test = test_data$total_UPDRS

# Fit model
ridge_model = glmnet(x_train, y_train, alpha = 0, lambda = 1)

# Make predictions
predictions_train <- predict(ridge_model, newx = x_train)
predictions_test <- predict(ridge_model, newx = x_test)

# R squared
SSR <- sum((predictions_train - y_train)^2)  # Sum of Squares of Residuals
SST <- sum((y_train - mean(y_train))^2)  # Total Sum of Squares
r_squared <- 1 - (SSR / SST)

# Adjusted R squared
n <- length(y_train)  # Number of observations
p <- ncol(x_train)  # Number of predictors (excluding intercept)
adj_r_squared <- 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))

# RMSE_train
rmse_train <- sqrt(mean((predictions_train - y_train)^2))

# RMSE_test
rmse_test_ridge <- sqrt(mean((predictions_test - y_test)^2))

# Extract the coefficients
coefficients <- coef(ridge_model)

# Extract the variable names
variables <- colnames(x_train)

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), r_squared, adj_r_squared, rmse_train, rmse_test_ridge)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")
```
alpha = 0 specifies Ridge regression (since alpha = 0 represents the L2 penalty), and lambda = 1 controls the amount of regularization. You can adjust the value of lambda to control the amount of shrinkage applied to the coefficients.

### (e) motor_UPDRS como respuesta

A value of "0.1" for both R2 and R2 adjusted means that the predictors included in the model explain approximately 10% of the variance in the dependent variable. This indicates a relatively weak relationship between the predictors and the response variable. Keep in mind that the interpretation of the R2 and R2 adjusted values depends on the specific context and the nature of the data being modeled.

Yo diria que si porque es puta mierda. No cumpliria el objetivo principal: El principal objetivo es predecir el UPDRS total a partir de las 16 medidas de voz. Pero esque este tampoco lo hace porque es basssurrra.

### (f) Análisis de residuos

In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable. 

```{r}
# Residual plot
plot(model_lineal, which = 1)

# Normal Q-Q plot
plot(model_lineal, which = 2)

# Scale-location plot (square root of standardized residuals)
plot(model_lineal, which = 3)

# Cook's distance
plot(model_lineal, which = 4)

# Residuals vs. fitted values plot
plot(model_lineal, which = 5)

```
    which = 1: Residuals vs. Fitted: Alrededor de 0, mas o menos lineal
    which = 2: Normal Q-Q plot: No son muy normal en linea recta
    which = 3: Scale-Location plot: homocedasticidad mas o menos, no es cono
    which = 4: Cook's distance plot: NO Hay puntos influyentes. Más grande cook más influye. Más 1 es influyente
    which = 5: Residuals vs. Leverage plot: No Existen puntos influyentes

Multicollinearity refers to a situation where independent variables in a regression model are highly correlated with each other. It can cause issues in the interpretation of coefficients and affect the stability and reliability of the regression model. To study multicollinearity in R, you can use the following approaches:

```{r}
cor_matrix <- cor(train_data[, c("Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")])

# install.packages("corrplot")
library(corrplot)
corrplot(cor_matrix, method = "color")

library(car)
vif_values <- vif(model_lineal)

tolerance_values <- 1/vif_values

# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_cor <- data.frame(
  VIF = vif_values,
  Tolerance = tolerance_values
)

print(results_cor)
```

Todos los tipos de Jitter están muy relacionados con todos los tipos de Jitter y los Shimmer con los Shimmer.Osea mucho kek. Normal que el modelo se una basurrra.

Tolerance is the reciprocal of the VIF. It indicates the proportion of variance in an independent variable that is not explained by other independent variables. Variables with low tolerance values (close to 0) indicate high multicollinearity.

The VIF measures how much the variance of the estimated regression coefficient is inflated due to multicollinearity.

# Ejercicio 3

### (a) Comparación modelos con y sin puntos influyentes

```{r}
# Calculate cooks distance
cooksd <- cooks.distance(model_lineal)

# Get 3 most influential points
top3_indices <- order(cooksd, decreasing = TRUE)[1:3]

# Remove the points
clean_train_data <- train_data[-top3_indices, ]

# Fit new model and calculate RMSE

# Lineal
# Fit the model
clean_model_lineal = lm(total_UPDRS ~ Jitter_p + Jitter_Abs + Jitter_RAP + Jitter_PPQ5 + Jitter_DDP + Shimmer + Shimmer_dB + Shimmer_APQ3 + Shimmer_APQ5 + Shimmer_APQ11 + Shimmer_DDA + NHR + HNR + RPDE + DFA + PPE, data = clean_train_data)
# Predict on the test data
predictions_test <- predict(clean_model_lineal, test_data)
# Calculate residuals
residuals_test <- test_data$total_UPDRS - predictions_test
# Calculate RMSE
rmse_clean_model_lineal <- sqrt(mean(residuals_test^2))

# Step
library(MASS)
# Perform stepwise variable selection based on AIC
clean_model_stepwise <- stepAIC(clean_model_lineal, direction = "both", trace = FALSE)
# Predict on the test data
predictions_test <- predict(clean_model_stepwise, test_data)
# Calculate residuals
residuals_test <- test_data$total_UPDRS - predictions_test
# Calculate RMSE
rmse_clean_model_step <- sqrt(mean(residuals_test^2))

# PCA
# Select predictor variables
predictor_vars = clean_train_data[7:22]
# Initialize variables for best model
clean_best_rmse <- Inf
clean_best_components <- 0
clean_best_model <- NULL
# Perform principal component analysis (PCA) on the predictor variables
pca <- prcomp(predictor_vars, scale = TRUE)
# Extract the principal components
pcs <- pca$x
# Identify the number of components with minimum RMSE
num_components <- 1:ncol(pcs)
rmse_values <- numeric(length(num_components))
for (i in num_components) {
  # Select the first i principal components
  pcs_selected <- pcs[, 1:i]
  
  # Create a regression model using the selected components
  model_pca <- lm(total_UPDRS ~ pcs_selected, data = clean_train_data)
  
  # Predict on the test data using the model
  predictions <- predict(model_pca, test_data)
  
  # Calculate residuals
  residuals <- test_data$total_UPDRS - predictions
  
  # Calculate RMSE
  rmse_values[i] <- sqrt(mean(residuals^2))
  
  # Check if current model has lower RMSE
  if (rmse_values[i] < best_rmse) {
    clean_best_rmse <- rmse_values[i]
    clean_best_components <- i
    clean_best_model <- model_pca
  }
}

# Ridge
# install.packages('glmnet')
library(glmnet)
# Select predictor variables as matrix
x_train = data.matrix(clean_train_data[7:22])
y_train = clean_train_data$total_UPDRS
x_test = data.matrix(test_data[7:22])
y_test = test_data$total_UPDRS
# Fit model
clean_ridge_model = glmnet(x_train, y_train, alpha = 0, lambda = 1)
# Make predictions
predictions_test <- predict(clean_ridge_model, newx = x_test)
# RMSE_test
cleanm_ridge_rmse <- sqrt(mean((predictions_test - y_test)^2))
# Extract the coefficients
coefficients <- coef(clean_ridge_model)
```
```{r}
# Create a data frame for the results
results <- data.frame(
  Modelo = c("OLS", "AIC", "RCP", "Ridge"),
  RMSE_Con = c(rmse_test_lineal, rmse_test_AIC, best_rmse, rmse_test_ridge),
  RMSE_Sin = c(rmse_clean_model_lineal, rmse_clean_model_step, clean_best_rmse, cleanm_ridge_rmse)
)

print(results)
```

```{r}
# Checking if the points were correctly deleted
# Re-calculate cooks distance
cooksd_updated <- cooks.distance(clean_model_lineal)

# Plot both
plot(cooksd_updated, pch = 20, cex = 1.5, main = "Cook's Distance Plot")
plot(cooksd, pch = 20, cex = 1.5, main = "Cook's Distance Plot")
```

### (b) Cálculo del RMSE robusto

The trimmed mean is less sensitive to outliers and can provide a more robust estimate of the central tendency.

```{r}
# Create a data frame for the results
results2 <- data.frame(
  Modelo = c("OLS", "AIC", "RCP", "Ridge"),
  RMSE_Con = c(rmse_test_lineal, rmse_test_AIC, best_rmse, rmse_test_ridge),
  RMSE_Sin = c(rmse_clean_model_lineal, rmse_clean_model_step, clean_best_rmse, cleanm_ridge_rmse),
  Robusto_Con = c(robust_rmse_lineal, ) ,
  Robusto_Sin = ,
)

print(results2)
```

### (c) LTS o Huber

# ANEXO

### Código