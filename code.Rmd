---
title: "PAC 2 Regresión Lineal"
author: "Maria Lucas"
date: "2023-06-22"
output: 
  word_document:
    toc: true
    toc_depth: 4
lang: Es-es
---
\newpage

# Ejercicio 1

Se ha procedido a la carga de los datos, asegurándonos de asignar el estatus de factor a las variables "diagnosis" y "age_cat" (Código 1).

```{r}
# Código 1: Carga de datos ejercicio 1

# Set viasulization number for results
options(scipen = 999)

# Set the file path
file_path <- "D:/Antiguos estudios/MASTER2/Sem2/Regresion/PAC2/P2/pancreas_biomarkers.txt"

# Load the data into a data frame
data <- read.table(file_path, header = TRUE, sep = "\t")

# Saving variables as factors
data$diagnosis <- factor(data$diagnosis, levels = 1:3)
data$age_cat <- factor(data$age_cat)
```

### (a) Modelo de regresión logística

Dado que nuestro objetivo principal consiste en distinguir entre pacientes diagnosticados con cáncer y aquellos con otras afecciones pancreáticas, se excluyeron del modelo los individuos que pertenecen al grupo de control. A continuación, se realizó un ajuste de regresión utilizando "diagnosis" como variable dependiente y las variables independientes "age_cat", "creatinina", "LYVE1", "REG1B" y "TFF1" (Código 2).

Aunque la inclusión de pacientes control en este caso no proporcionaría información específica sobre las diferencias entre los grupos de pacientes con cáncer y aquellos con afecciones no cancerosas, su exclusión del estudio plantea ciertas limitaciones. Entre estas limitaciones se destaca la posible falta de generalización de los resultados a la población general, así como la dificultad para evaluar la sensibilidad o especificidad del modelo.

Es importante tener en cuenta que al aplicar el modelo, se debe hacerlo con precaución y limitarlo a casos en los que se tenga conocimiento de una afectación pancreática, pero no se disponga de información sobre si es de naturaleza cancerosa. No se deben incluir pacientes sanos en la aplicación del modelo, ya que este no ha sido construido con los datos necesarios para tal propósito.

```{r}
# Código 2: Ajuste regresión logística ejercicio 1a

# Subset the data to include only levels 2 and 3 of the "diagnosis" variable
subset_data <- subset(data, diagnosis != 1)

# Recode values 2 and 3 to 0 and 1, respectively
subset_data$diagnosis <- ifelse(subset_data$diagnosis == 2, 0, 1)

# Fit the logistic regression model using the subsetted data
model <- glm(diagnosis ~ age_cat + creatinine + LYVE1 + REG1B + TFF1, data = subset_data, family = binomial)

# View the model summary
summary(model)
```

Los resultados obtenidos revelan que la edad, a partir de los 56 años, constituye un indicador significativo. Asimismo, se destaca que tanto LYVE1 como REG1B son variables relevantes para la predicción del riesgo de adenocarcinoma ductal pancreático. Esto se evidencia en el hecho de que todas estas variables presentan un valor p inferior a 0.05. En el contexto de la regresión logística, un valor p de 0.05 permite rechazar la hipótesis nula de ausencia de relación entre la variable predictora y la variable respuesta. En otras palabras, dichas variables tienen un impacto significativo sobre la clase.

Es importante resaltar que la significancia del intercepto sugiere que parte de la variabilidad de la variable respuesta no puede ser explicada por las variables independientes consideradas en nuestro estudio. En la regresión logística, el intercepto captura la probabilidad de que ocurra un evento cuando todas las variables predictoras se encuentran en su nivel de referencia. En otras palabras, la significancia del intercepto implica que existen diferencias significativas entre las clases, incluso en ausencia de cualquier predictor.

En conclusión, la edad (especialmente a partir de los 56 años), LYVE1 y REG1B son las variables que permiten predecir el riesgo de cáncer pancreático, brindando así la capacidad de distinguir entre otras afecciones pancreáticas no cancerosas.

### (b) Interpretación de coeficientes

En un modelo de regresión logística, los coeficientes indican el cambio estimado en el logaritmo de las probabilidades (log-odds) del evento en estudio (en este caso, la presencia de adenocarcinoma ductal pancreático, codificado como 1) asociado a un cambio unitario en la variable predictora, manteniendo constantes las demás variables.

Un coeficiente estimado positivo sugiere que un incremento en el valor de la variable está asociado con un aumento en la probabilidad (log-odds) del evento en estudio. Por ejemplo, un coeficiente de 0.31 indica que un incremento de 1 en el valor del predictor se asocia con un aumento de 0.31 en el log-odds del evento en estudio. Este es el caso de LYVE1, donde un aumento en el valor de LYVE1 está relacionado con una mayor probabilidad de tener adenocarcinoma (0.3140). De manera similar, la edad también parece ser un factor influyente. Tener más de 56 años incrementa el log-odds de tener adenocarcinoma, y dicho incremento varía según los rangos de edad: de 56 a 65 años (3.032), de 66 a 75 años (2.7) y a partir de los 75 años (3.443).

A simple vista, podría parecer que la edad tiene un impacto mayor en la probabilidad de presentar adenocarcinoma en comparación con LYVE1 (0.31 < 2.7-3.4). Sin embargo, es importante tener en cuenta que esta comparación se basa en un aumento de 1 en el valor de la edad o LYVE1. La interpretación de la magnitud de los coeficientes estimados debe realizarse con cautela, ya que depende de la escala y las unidades de medida de cada variable. Además, es importante considerar que la escala de las variables puede influir en la magnitud de los coeficientes estimados y, por lo tanto, debe tenerse en cuenta al interpretarlos.

Por otro lado, un coeficiente estimado negativo indica que un aumento en el valor de la variable está asociado con una disminución en el log-odds del evento en estudio (tener adenocarcinoma).

### (c) Modelo reducido

Para realizar una comparación entre ambos modelos, podemos aplicar un análisis de varianza (ANOVA) utilizando el test de Chi cuadrado (Código 3). Con esto, evaluamos el ajuste de cada modelo bajo las siguientes hipótesis:

- **H0:** Desviación del modelo reducido = Desviación del modelo completo. No hay diferencia de ajuste entre los modelos.
- **H1:** Desviación del modelo reducido > Desviación del modelo completo. El modelo reducido presenta un ajuste deficiente en comparación con el modelo completo.

La desviación del modelo es una medida de la discrepancia entre los valores observados de los datos y los valores predichos por el modelo.

```{r}
# Código 3: Anova del modelo reducido vs completo, ejercicio 1c 

# Fit the logistic regression model using the subsetted data and indicated variables
model_simple <- glm(diagnosis ~ age_cat + LYVE1 + REG1B, data = subset_data, family = binomial)

summary(model_simple)

# Compare the two models
anova(model, model_simple, test = "Chi")
```

Al observar el resultado del p-valor (0.31), no encontramos evidencia significativa en contra de la hipótesis nula. Por lo tanto, aceptamos la hipótesis nula y concluimos que el modelo reducido (sin creatinina y sin TFF1) tiene un ajuste comparable al modelo completo. Además, al comparar los valores del criterio de información de Akaike (AIC), observamos que el AIC del modelo reducido es 2 unidades menor que el del modelo completo. Esto sugiere que el modelo reducido tiene un mejor ajuste, teniendo en cuenta la complejidad de ambos modelos.

### (d) Funcion cuadrática

Ajustamos dos modelos adicionales, uno que incluye el término cuadrático de LYVE1 y otro que incluye el término cuadrático de REB1B. Posteriormente, realizamos un análisis de varianza (ANOVA) para comparar cada modelo con su versión reducida (sin los términos cuadráticos) (Código 4).

```{r}
# Codigo 4: Suposición de linealidad mediante la adición de términos cuadráticos, ejercicio 1d

# Cuadratic LYVE1
model_simple_LYVE1 = glm(diagnosis ~ age_cat + LYVE1 + I(LYVE1^2) + REG1B, data = subset_data, family = binomial)

summary(model_simple_LYVE1)

# Compare the two models
anova(model_simple, model_simple_LYVE1, test = "Chi")

# Cuadratic REG1B
model_simple_REG1B = glm(diagnosis ~ age_cat + LYVE1 + REG1B + I(REG1B^2), data = subset_data, family = binomial)

summary(model_simple_REG1B)

# Compare the two models
anova(model_simple, model_simple_REG1B, test = "Chi")
```

En ambos casos, al añadir los términos cuadráticos, no se observó una mejora significativa en el ajuste del modelo. Siguiendo la explicación anterior, los valores de p fueron 0.3 y 0.4, respectivamente, lo que nos lleva a aceptar la hipótesis nula en ambos casos. Esto implica que los modelos con y sin los términos cuadráticos tienen un ajuste similar.

Además, al examinar los valores del criterio de información de Akaike (AIC), se observa que los modelos con los términos cuadráticos presentan un AIC superior. Un valor de AIC más alto sugiere que el modelo no se ajusta mejor teniendo en cuenta la complejidad añadida por los términos cuadráticos.

En conclusión, no se recomienda incluir ninguno de los términos cuadráticos en los modelos. Es importante destacar que, aunque los resultados de los tests no sean significativos, siempre se deben evaluar en conjunto con el contexto del análisis. En este caso, se realizó una inspección visual adicional de la relación entre las variables y el log-odds de la variable respuesta (disponible en el ANEXO, Código 5). Estos gráficos confirman que existe una relación lineal, ya que se observa un patrón lineal sin la presencia de curvas, forma de U u otros patrones no lineales.

```{r}
# Código 5: variable vs log odds para linealidad del ejercicio 1d

# Obtain predicted log odds from the model
predicted_logodds <- predict(model_simple, type = "link")

plot(subset_data$LYVE1, predicted_logodds, xlab = "age_cat", ylab = "Log Odds", main = "Scatter plot - LYVE1 vs. Log Odds")

plot(subset_data$REG1B, predicted_logodds, xlab = "age_cat", ylab = "Log Odds", main = "Scatter plot - REG1B vs. Log Odds")
```

### (e) Predicción de caso

Guardamos los datos del paciente con los valores indicados en el enunciado. Dado que la edad del paciente es de 68 años, esta cae dentro del rango de 66 a 75 años, por lo que se almacena en la variable "age_cat" con dicho rango. El resto de las variables se guardan utilizando los valores numéricos correspondientes. A continuación, aplicamos la función "predict" al modelo reducido utilizando estos datos y especificamos el tipo de respuesta como "response" para obtener la probabilidad con la que el caso ha sido clasificado (Código 6).

```{r}
# Código 6: Predicción de caso para el ejercicio 1e

# Create a new data frame for the new case
new_case = data.frame(age_cat = "66-75", LYVE1 = 6, REG1B = 140)

# Predict the outcome using the model
prediction = predict(model_simple, newdata = new_case, type = "response")

cat("Tipo de afección: ", names(prediction), "\n")
cat("Probabilidad de la clasificación: ", prediction[1])
```
El modelo predice la presencia de adenocarcinoma ductal pancreático con una probabilidad del 72,42%. Cabe destacar que los valores fueron recodificados de la siguiente manera: 0 representa afecciones pancreáticas no cancerosas y 1 representa adenocarcinoma ductal pancreático.

Es importante tener en cuenta que la extrapolación se produce cuando se realizan predicciones para datos de la variable predictora que se encuentran fuera del rango de los datos utilizados para construir el modelo. En este caso, es necesario considerar que la predicción se basa en los datos y el rango utilizados durante el entrenamiento del modelo, por lo que las predicciones para valores fuera de ese rango pueden ser menos precisas o no reflejar adecuadamente la realidad.

Con el propósito de verificar la idoneidad del modelo, hemos examinado los rangos de las variables LYVE1 y REG1B. En el código implementado, se ha incorporado una comparación automática que determina si los valores pertenecen o no a los rangos establecidos, utilizando un valor booleano. Además, se han generado gráficos que representan los datos utilizados en la construcción del modelo para cada variable, resaltando en color rojo el caso de estudio en cuestión (Código 7).

```{r}
# Código 7: Comprobación de extrapolación para el caso del ejercicio 1e

# Check predictor variable ranges
range_data <- sapply(subset_data[, c("LYVE1", "REG1B")], range)
range_new_observation <- c(6, 140)  # Replace with the values of the new observation

# Compare new observation values with observed range
is_extrapolation <- any(range_new_observation < range_data[1, ] | range_new_observation > range_data[2, ])

# Print results
cat("Extrapolación:", is_extrapolation, "\n")

# Assess distribution of predictor variables
# LYVE1
hist(subset_data$LYVE1, main = "Distribution of LYVE1")
# Add new observation to LYVE1 plot
points(6, 0, col = "red", pch = 16)

# REGB1
hist(subset_data$REG1B, main = "Distribution of REG1B")
# Add new observation to REGB1 plot
points(140, 0, col = "red", pch = 16)

# age_cat
barplot(table(subset_data$age_cat), main = "Distribution of age_cat", xlab = "age_cat", ylab = "Frequency")
# Add new observation to age_cat plot
points(5.5, 0, col = "red", pch = 16)
```

Como podemos observar, los valores correspondientes al caso que deseamos predecir se encuentran dentro de los rangos utilizados en la construcción del modelo para las tres variables.

Es relevante destacar que las variables creatinina y TFF1 no han sido incluidas en el modelo debido a que hemos determinado que el modelo reducido, es decir, aquel sin la inclusión de estas variables, presenta un ajuste igual o mejor que el modelo completo. Del mismo modo, podemos aplicar el modelo en este caso particular, dado que se trata de un paciente con una afección pancreática que requerimos clasificar como cancerosa o no cancerosa. Como hemos explicado previamente, si se tratara de un paciente sano, no podríamos aplicar este modelo con la misma certeza.

# Ejercicio 2

```{r}
# Código 8: Carga de datos para el ejercicio 2 y 3

set.seed(123) # Seed is fixed multiple times in the code because it somehow was needed

# Data import
# Note: I changed the variable names to avoid problems with symbols

import.data <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data"
data <- read.table(url(import.data), sep=",", skip=1)
names(data) <- c("subject#","age","sex","test_time","motor_UPDRS","total_UPDRS",
"Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")

# Select predictor variables and response
library(dplyr)
parkinson <- data %>% dplyr::select(6:22)

# Split the data into train and test
library(caret)
set.seed(123)
train_indices <- createDataPartition(parkinson$total_UPDRS, p = 0.8, list = FALSE)
train_data <- parkinson[train_indices, ]
test_data <- parkinson[-train_indices, ]
```

### (a) Regresión lineal

Al no utilizar el factor "sujeto" no tenemos en cuenta las posibles variaciones de cada individuo. Las muestras deberían ser apareadas, ya que se ace un seguimiento a lo largo del tiempo. Una estimación. No sé lo que digo ya lo mirare.

```{r}
# Fit the model
model_lineal = lm(total_UPDRS ~ ., data = train_data)

# Extract R-squared
r_squared = summary(model_lineal)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_lineal)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_lineal, train_data)
# Predict on the test data
predictions_test <- predict(model_lineal, test_data)

# Calculate residuals train
residuals_train = train_data$total_UPDRS - predictions_train
# Calculate residuals test
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate RMSE train
lineal_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
lineal_rmse_test <- sqrt(mean(residuals_test^2))

# Create results table
# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), r_squared, adj_r_squared, lineal_rmse_train, lineal_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for exercise 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean
robust_lineal_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```

Al no considerar el "sujeto" se viola la suposición de independencia. En este tipo de modelos, se asume que todas las observaciones son independientes. Al no serlo, se construirá un modelo que incluirá métricas erróneas. De manera similar, se pierde precisión al no tener en cuenta la correlación entre los datos. El error estándar de los coeficientes estimados puede subestimarse, dando intervalos de confianza más estrechos.

Adicionalmente, se aumenta el error de Tipo 1 (rechazar incorrectamente H0 haciendo que una variable sea significativa cuando no lo es). Esto sucede porque los datos de un mismo individuo tienden a ser más similares, inflando así la significación de los resultados. 

Finalmente, se hace más complicado detectar las variaciones entre en un mismo individuo. 

### (b) Regresión lineal con AIC

```{r}
library(MASS)

set.seed(123)

# Perform stepwise variable selection based on AIC
model_stepwise <- stepAIC(model_lineal, direction = "both", trace = FALSE)

# Extract R-squared
r_squared = summary(model_stepwise)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_stepwise)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_stepwise, train_data)
# Predict on the test data
predictions_test <- predict(model_stepwise, test_data)

# Calculate residuals
residuals_train = train_data$total_UPDRS - predictions_train
# Calculate residuals
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate RMSE train
step_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
step_rmse_test <- sqrt(mean(residuals_test^2))

# Results
# Extract the variable names from the linear regression model
variables <- names(coef(model_stepwise))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), r_squared, adj_r_squared, step_rmse_train, step_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_step_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```
Removing a variable solely based on its lack of significance may affect the overall fit of the model and the relationships between other variables. A variable that is not individually significant may contribute to the model's overall predictive power when combined with other variables or interacted with other predictors. Therefore, it's crucial to assess the model's overall performance, such as through measures like R-squared or adjusted R-squared, and consider the context and theoretical significance of the variables

The significance of a variable can be influenced by multicollinearity, which occurs when predictor variables are highly correlated with each other. In the presence of multicollinearity, the individual coefficients and their significance can be unstable or misleading. It's important to check for multicollinearity among the variables and consider its potential impact on the significance of individual predictors.

In this example, model_linear is the initial linear regression model you built using all the variables of interest. The stepAIC() function from the MASS package is used to perform the stepwise variable selection based on AIC.

The direction argument specifies the direction of the stepwise procedure. "both" allows variables to be added or removed from the model. Other options include "backward" for variable removal only and "forward" for variable addition only.

### (c) Regresión por componentes principales

Performing regression using principal components involves transforming the predictor variables into a set of principal components and then using these components as predictors in the regression model.

```{r}
# install.packages('pls')
library(pls)

set.seed(123)

PCA_model <- pcr(total_UPDRS ~ ., data = train_data, validation="CV", scale = TRUE)

# Calculate RMSE values
mpcCV <- RMSEP(PCA_model, estimate = "CV")
rmse_values <- round(mpcCV$val, 1) # Rounded to first decimal to account for complexity

# Plot the graph
plot(mpcCV$comp, rmse_values, type = "b", xlab = "Number of Components", ylab = "RMSE")

# Find the optimal number of components
optimal_components <- mpcCV$comp[which.min(rmse_values)]
points(optimal_components, min(rmse_values), col = "red", pch = 16)
text(optimal_components, min(rmse_values), paste("Optimal:", optimal_components), pos = 3)
```
El minimo absoluto es 11 pero el minimo razonable sin hacerlo muy complejo es 8 -1 = 7

In the code you provided, the line (numpredcp <- which.min(mpcCV$val)) is used to find the number of principal components that minimizes the cross-validated RMSEP (mpcCV$val). This line calculates the RMSEP for different numbers of components using cross-validation and then identifies the index of the minimum value using the which.min function.

The code performs principal component analysis (PCA) on the predictor variables using the prcomp() function. The resulting principal components (pcs) are then used to construct regression models with different numbers of components (ranging from 1 to the total number of components).

For each model, the code predicts the outcome variable on the test data, calculates the residuals, and computes the root mean squared error (RMSE). The RMSE values are stored in the rmse_values vector.

The code then plots the RMSE values against the number of components to visualize the relationship. The number of components that yields the minimum RMSE is identified, and the corresponding results are printed.

Yes, in general, a lower RMSE indicates a better-fitting model. RMSE (Root Mean Squared Error) is a commonly used measure of the average prediction error of a regression model. It represents the square root of the average squared differences between the predicted values and the actual values of the outcome variable.

Since RMSE measures the magnitude of the prediction errors, a smaller RMSE implies that the model's predictions are, on average, closer to the actual values. 

```{r}
# Predict on the training data using the model
predictions_train <- predict(PCA_model, train_data, ncomp = optimal_components-1)
# Predict on the test data using the model
predictions_test <- predict(PCA_model, test_data, ncomp = optimal_components-1)
  
# Calculate residuals for training and test data
residuals_train <- train_data$total_UPDRS - predictions_train
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate R-squared for training data
r2_train <- 1 - sum(residuals_train^2) / sum((train_data$total_UPDRS - mean(train_data$total_UPDRS))^2)

# Calculate adjusted R-squared for training data
n_train <- nrow(train_data)
p_train <- optimal_components - 1  # Number of predictors (components) used
r2_adj_train <- 1 - (1 - r2_train) * ((n_train - 1) / (n_train - p_train - 1))
  
# Calculate RMSE for training data
PCA_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
PCA_rmse_test <- sqrt(mean(residuals_test^2))

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c((optimal_components - 1), r2_train, r2_adj_train, PCA_rmse_train, PCA_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for ex3
# Number of observations to trim
n_trim_best <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_best <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_best <- sorted_residuals_best[(n_trim_best + 1):(length(residuals_test) - n_trim_best)]
# Calculate the trimmed mean of the residuals for the best model
trimmed_mean_best <- mean(trimmed_residuals_best)
# Calculate the squared residuals using the trimmed mean for the best model
trimmed_squared_residuals_best <- (trimmed_residuals_best - trimmed_mean_best)^2
# Calculate the robust RMSE using the trimmed mean for the best model
robust_PCA_rmse_test <- sqrt(mean(trimmed_squared_residuals_best))
```

Nota: el mejor modelo según el RMSEP con CV, que no se que es.

### (d) Ridge regression

Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.

Yes, you can adjust the model using Ridge regression. Ridge regression is a regularization technique that introduces a penalty term to the least squares objective function, helping to reduce the impact of multicollinearity and potentially improve the model's performance.

```{r}
library(MASS)

set.seed(123)

mr <- lm.ridge(total_UPDRS ~ ., data = train_data, lambda=(seq(0, 0.1, 0.001)))
(nGCV <- which.min(mr$GCV))

lGCV <- mr$lambda[nGCV]
matplot(mr$lambda,coef(mr),type="l", ylim=c(-2,2), xlab=expression(lambda),ylab=expression(hat(beta[i])))
abline(v=lGCV,col=2)

plot(mr$lambda,mr$GCV,type="l",xlab=expression(lambda),ylab="GCV")
abline(v=lGCV,col=2)

mr <- lm.ridge(total_UPDRS ~ ., data = train_data, lambda=lGCV)
```

```{r}
# Make predictions (no y this time)
predictions_test = cbind(1,as.matrix(test_data[,-1])) %*% coef(mr)
predictions_train = cbind(1,as.matrix(train_data[,-1])) %*% coef(mr)

# Calculate residuals for training and test data
residuals_train <- train_data$total_UPDRS - predictions_train
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate R-squared for training data
r2_train <- 1 - sum(residuals_train^2) / sum((train_data$total_UPDRS - mean(train_data$total_UPDRS))^2)

# Calculate adjusted R-squared for training data
n_train <- nrow(train_data)
p_train <- optimal_components - 1  # Number of predictors (components) used
r2_adj_train <- 1 - (1 - r2_train) * ((n_train - 1) / (n_train - p_train - 1))
  
# Calculate RMSE for training data
ridge_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
ridge_rmse_test <- sqrt(mean(residuals_test^2))

# Extract the variable names
variables <- colnames(test_data[,-1])

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c((optimal_components - 1), r2_train, r2_adj_train, ridge_rmse_train, ridge_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for ex 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_ridge_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```


In the context of ridge regression, lambda (λ) is a regularization parameter that controls the amount of shrinkage applied to the regression coefficients. It is also referred to as the penalty parameter or the tuning parameter.


Ridge regression is a technique used to address the issue of multicollinearity (high correlation) among predictor variables in a regression model. When multicollinearity is present, the ordinary least squares (OLS) estimates become unstable, leading to overfitting and unreliable coefficient estimates.

Lambda plays a crucial role in ridge regression by introducing a penalty term to the loss function that the model tries to minimize. The penalty term is proportional to the square of the magnitude of the coefficients. By increasing the value of lambda, the ridge regression model imposes a stronger penalty, shrinking the coefficient estimates towards zero.

By tuning the value of lambda, you can control the degree of shrinkage applied to the coefficients. A larger lambda value corresponds to stronger regularization and more pronounced shrinkage of the coefficients. Conversely, a smaller lambda value reduces the amount of shrinkage and allows the model to closely approximate the OLS estimates.

### (e) motor_UPDRS como respuesta

A value of "0.1" for both R2 and R2 adjusted means that the predictors included in the model explain approximately 10% of the variance in the dependent variable. This indicates a relatively weak relationship between the predictors and the response variable. Keep in mind that the interpretation of the R2 and R2 adjusted values depends on the specific context and the nature of the data being modeled.

Yo diria que si porque es puta mierda. No cumpliria el objetivo principal: El principal objetivo es predecir el UPDRS total a partir de las 16 medidas de voz. Pero esque este tampoco lo hace porque es basssurrra.

### (f) Análisis de residuos

In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable. 

```{r}
# Residual plot
plot(model_lineal, which = 1)

# Normal Q-Q plot
plot(model_lineal, which = 2)

# Scale-location plot (square root of standardized residuals)
plot(model_lineal, which = 3)

# Cook's distance
plot(model_lineal, which = 4)

# Residuals vs. fitted values plot
plot(model_lineal, which = 5)
```
    which = 1: Residuals vs. Fitted: Alrededor de 0, mas o menos lineal
    which = 2: Normal Q-Q plot: No son muy normal en linea recta
    which = 3: Scale-Location plot: homocedasticidad mas o menos, no es cono
    which = 4: Cook's distance plot: NO Hay puntos influyentes. Más grande cook más influye. Más 1 es influyente
    which = 5: Residuals vs. Leverage plot: No Existen puntos influyentes

Multicollinearity refers to a situation where independent variables in a regression model are highly correlated with each other. It can cause issues in the interpretation of coefficients and affect the stability and reliability of the regression model. To study multicollinearity in R, you can use the following approaches:

```{r}
cor_matrix <- cor(train_data[, c("Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")])

# install.packages("corrplot")
library(corrplot)
corrplot(cor_matrix, method = "color")

library(car)
vif_values <- vif(model_lineal)

tolerance_values <- 1/vif_values

# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_cor <- data.frame(
  VIF = vif_values,
  Tolerance = tolerance_values
)

print(results_cor)
```

Todos los tipos de Jitter están muy relacionados con todos los tipos de Jitter y los Shimmer con los Shimmer.Osea mucho kek. Normal que el modelo se una basurrra.

Tolerance is the reciprocal of the VIF. It indicates the proportion of variance in an independent variable that is not explained by other independent variables. Variables with low tolerance values (close to 0) indicate high multicollinearity.

The VIF measures how much the variance of the estimated regression coefficient is inflated due to multicollinearity.

```{r}
# Calculate residuals
residuals <- residuals(model_lineal)

# Plot residuals
plot(residuals[-length(residuals)], residuals[-1], xlab = "Residual i", ylab = "Residual i+1", main = "Correlation between residuals")

```

If your data is randomly ordered and does not have a time component, then the concept of "consecutive residuals" in the temporal sense may not be directly applicable. The assumption of independence between residuals in linear regression typically assumes that the order of the data points does not matter, as long as the observations are independent and identically distributed.

In this case, you can still check for correlation between the residuals, but the interpretation would be different. Instead of investigating temporal dependence, you would be examining whether there is a pattern or relationship between the residuals regardless of their order. The correlation between residuals can provide insights into potential systematic patterns or relationships in the errors that may affect the validity of your linear model.

Therefore, while the concept of consecutive residuals in a temporal sense may not be relevant to your randomly ordered data, you can still use the code you provided to check for correlation between the residuals and investigate whether there are correlated errors present in your linear model.

# Ejercicio 3

### (a) Comparación modelos con y sin puntos influyentes

```{r}
# Calculate cooks distance
cooksd <- cooks.distance(model_lineal)

# Get 3 most influential points
top3_indices <- order(cooksd, decreasing = TRUE)[1:3]

# Remove the points
train_data <- train_data[-top3_indices, ]
```

Re-execute everything but with clean data

```{r}
# Fit the model
model_lineal = lm(total_UPDRS ~ ., data = train_data)

# Extract R-squared
r_squared = summary(model_lineal)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_lineal)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_lineal, train_data)
# Predict on the test data
predictions_test <- predict(model_lineal, test_data)

# Calculate residuals train
residuals_train = train_data$total_UPDRS - predictions_train
# Calculate residuals test
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate RMSE train
lineal_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
lineal_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Create results table
# Extract the variable names from the linear regression model
variables <- names(coef(model_lineal))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c(length(variables), r_squared, adj_r_squared, lineal_rmse_train, lineal_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for exercise 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean
robust_lineal_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```
________________________________________
```{r}
library(MASS)

set.seed(123)

# Perform stepwise variable selection based on AIC
model_stepwise <- stepAIC(model_lineal, direction = "both", trace = FALSE)

# Extract R-squared
r_squared = summary(model_stepwise)$r.squared

# Extract adjusted R-squared
adj_r_squared = summary(model_stepwise)$adj.r.squared

# Predict on the training data
predictions_train = predict(model_stepwise, train_data)
# Predict on the test data
predictions_test <- predict(model_stepwise, test_data)

# Calculate residuals
residuals_train = train_data$total_UPDRS - predictions_train
# Calculate residuals
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate RMSE train
step_rmse_train = sqrt(mean(residuals_train^2))
# Calculate RMSE test
step_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Results
# Extract the variable names from the linear regression model
variables <- names(coef(model_stepwise))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c(length(variables), r_squared, adj_r_squared, step_rmse_train, step_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_step_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))

mean(residuals_test, trim=0.1)
```
______________________________________________________________________
```{r}
# install.packages('pls')
library(pls)

set.seed(123)

PCA_model <- pcr(total_UPDRS ~ ., data = train_data, validation="CV", scale = TRUE)

# Calculate RMSE values
mpcCV <- RMSEP(PCA_model, estimate = "CV")
rmse_values <- round(mpcCV$val, 1) # Rounded to first decimal to account for complexity

# Plot the graph
plot(mpcCV$comp, rmse_values, type = "b", xlab = "Number of Components", ylab = "RMSE")

# Find the optimal number of components
optimal_components <- mpcCV$comp[which.min(rmse_values)]
points(optimal_components, min(rmse_values), col = "red", pch = 16)
text(optimal_components, min(rmse_values), paste("Optimal:", optimal_components), pos = 3)
```
```{r}
# Predict on the training data using the model
predictions_train <- predict(PCA_model, train_data, ncomp = optimal_components-1)
# Predict on the test data using the model
predictions_test <- predict(PCA_model, test_data, ncomp = optimal_components-1)
  
# Calculate residuals for training and test data
residuals_train <- train_data$total_UPDRS - predictions_train
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate R-squared for training data
r2_train <- 1 - sum(residuals_train^2) / sum((train_data$total_UPDRS - mean(train_data$total_UPDRS))^2)

# Calculate adjusted R-squared for training data
n_train <- nrow(train_data)
p_train <- optimal_components - 1  # Number of predictors (components) used
r2_adj_train <- 1 - (1 - r2_train) * ((n_train - 1) / (n_train - p_train - 1))
  
# Calculate RMSE for training data
PCA_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
PCA_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c((optimal_components - 1), r2_train, r2_adj_train, PCA_rmse_train, PCA_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for ex3
# Number of observations to trim
n_trim_best <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_best <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_best <- sorted_residuals_best[(n_trim_best + 1):(length(residuals_test) - n_trim_best)]
# Calculate the trimmed mean of the residuals for the best model
trimmed_mean_best <- mean(trimmed_residuals_best)
# Calculate the squared residuals using the trimmed mean for the best model
trimmed_squared_residuals_best <- (trimmed_residuals_best - trimmed_mean_best)^2
# Calculate the robust RMSE using the trimmed mean for the best model
robust_PCA_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_best))
```
______________________________________________________________________
```{r}
library(MASS)

set.seed(123)

mr <- lm.ridge(total_UPDRS ~ ., data = train_data, lambda=(seq(0, 0.1, 0.001)))
(nGCV <- which.min(mr$GCV))

lGCV <- mr$lambda[nGCV]
matplot(mr$lambda,coef(mr),type="l", ylim=c(-2,2), xlab=expression(lambda),ylab=expression(hat(beta[i])))
abline(v=lGCV,col=2)

plot(mr$lambda,mr$GCV,type="l",xlab=expression(lambda),ylab="GCV")
abline(v=lGCV,col=2)

mr <- lm.ridge(total_UPDRS ~ ., data = train_data, lambda=lGCV)
```
```{r}
# Make predictions (no y this time)
predictions_test = cbind(1,as.matrix(test_data[,-1])) %*% coef(mr)
predictions_train = cbind(1,as.matrix(train_data[,-1])) %*% coef(mr)

# Calculate residuals for training and test data
residuals_train <- train_data$total_UPDRS - predictions_train
residuals_test <- test_data$total_UPDRS - predictions_test

# Calculate R-squared for training data
r2_train <- 1 - sum(residuals_train^2) / sum((train_data$total_UPDRS - mean(train_data$total_UPDRS))^2)

# Calculate adjusted R-squared for training data
n_train <- nrow(train_data)
p_train <- optimal_components - 1  # Number of predictors (components) used
r2_adj_train <- 1 - (1 - r2_train) * ((n_train - 1) / (n_train - p_train - 1))
  
# Calculate RMSE for training data
ridge_rmse_train <- sqrt(mean(residuals_train^2))
# Calculate RMSE for test data
ridge_clean_rmse_test <- sqrt(mean(residuals_test^2))

# Extract the variable names
variables <- colnames(test_data[,-1])

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de componentes", "R-squared", "Adjusted R-squared", "RMSE_train", "clean_rmse_test"),
  Value = c((optimal_components - 1), r2_train, r2_adj_train, ridge_rmse_train, ridge_clean_rmse_test)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")

# Robust RMSE for ex 3
# Number of observations to trim
n_trim <- round(0.1 * length(residuals_test))
# Sort the residuals in ascending order
sorted_residuals_test <- sort(residuals_test)
# Trim the specified percentage of observations from both ends
trimmed_residuals_test <- sorted_residuals_test[(n_trim + 1):(length(residuals_test) - n_trim)]
# Calculate the trimmed mean of the residuals
trimmed_mean_test <- mean(trimmed_residuals_test)
# Calculate the squared residuals using the trimmed mean:
trimmed_squared_residuals_test <- (trimmed_residuals_test - trimmed_mean_test)^2
# Calculate the robust RMSE using the trimmed mean:
robust_ridge_clean_rmse_test <- sqrt(mean(trimmed_squared_residuals_test))
```

```{r}
# Create a data frame for the results
results <- data.frame(
  Modelo = c("OLS", "AIC", "RCP", "Ridge"),
  RMSE_Con = c(lineal_rmse_test, step_rmse_test, PCA_rmse_test, ridge_rmse_test),
  RMSE_Sin = c(lineal_clean_rmse_test, step_clean_rmse_test, PCA_clean_rmse_test, ridge_clean_rmse_test)
)

print(results)
```

```{r}
# Checking if the points were correctly deleted
# Re-calculate cooks distance
cooksd_updated <- cooks.distance(model_lineal) # Clean data

# Plot both
plot(cooksd_updated, pch = 20, cex = 1.5, main = "Cook's Distance Plot Updated")
plot(cooksd, pch = 20, cex = 1.5, main = "Cook's Distance Plot")
```

### (b) Cálculo del RMSE robusto

The trimmed mean is less sensitive to outliers and can provide a more robust estimate of the central tendency.

```{r}
# Create a data frame for the results
results2 <- data.frame(
  Modelo = c("OLS", "AIC", "RCP", "Ridge"),
  RMSE_Con = c(lineal_rmse_test, step_rmse_test, PCA_rmse_test, ridge_rmse_test),
  RMSE_Sin = c(lineal_clean_rmse_test, step_clean_rmse_test, PCA_clean_rmse_test, ridge_clean_rmse_test),
  Robusto_Con = c(robust_lineal_rmse_test, robust_step_rmse_test, robust_PCA_rmse_test, robust_ridge_rmse_test),
  Robusto_Sin = c(robust_lineal_clean_rmse_test, robust_step_clean_rmse_test, robust_PCA_clean_rmse_test, robust_ridge_clean_rmse_test)
)

print(results2)
```

### (c) LTS o Huber

```{r}
set.seed(123)

# Data import
# Note: I changed the variable names to avoid problems with symbols

import.data <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data"
parkinson <- read.table(url(import.data), sep=",", skip=1)
names(parkinson) <- c("subject#","age","sex","test_time","motor_UPDRS","total_UPDRS",
"Jitter_p","Jitter_Abs","Jitter_RAP","Jitter_PPQ5","Jitter_DDP",
"Shimmer","Shimmer_dB","Shimmer_APQ3","Shimmer_APQ5","Shimmer_APQ11", "Shimmer_DDA","NHR","HNR","RPDE","DFA","PPE")

# Select predictor variables and response
library(dplyr)
set.seed(123)
parkinson <- parkinson %>% dplyr::select(6:22)

# Split the data into train and test
library(caret)
set.seed(123)
train_indices <- createDataPartition(parkinson$total_UPDRS, p = 0.8, list = FALSE)
train_data <- parkinson[train_indices, ]
test_data <- parkinson[-train_indices, ]

# Huber
require(MASS)
hub <- rlm(total_UPDRS ~ ., data = train_data)
summary(hub)

# Predict on train and test data
train_pred <- predict(hub, newdata = train_data)
test_pred <- predict(hub, newdata = test_data)

# Calculate R-squared
train_r2 <- 1 - sum((train_data$total_UPDRS - train_pred)^2) / sum((train_data$total_UPDRS - mean(train_data$total_UPDRS))^2)
test_r2 <- 1 - sum((test_data$total_UPDRS - test_pred)^2) / sum((test_data$total_UPDRS - mean(train_data$total_UPDRS))^2)

# Calculate adjusted R-squared
n <- nrow(train_data)
p <- length(coef(hub))
train_r2_adj <- 1 - (1 - train_r2) * ((n - 1) / (n - p - 1))
test_r2_adj <- 1 - (1 - test_r2) * ((n - 1) / (n - p - 1))

# Calculate RMSE
train_rmse <- sqrt(mean((train_data$total_UPDRS - train_pred)^2))
test_rmse <- sqrt(mean((test_data$total_UPDRS - test_pred)^2))

# Create results table
# Extract the variable names from the linear regression model
variables <- names(coef(hub))[-1]

# Create a data frame for the results
results_table <- data.frame(
  Metric = c("Número de variables", "R-squared", "Adjusted R-squared", "RMSE_train", "RMSE_test"),
  Value = c(length(variables), train_r2, train_r2_adj, train_rmse, test_rmse)
)

# Print the result table
print(results_table)
cat("Variables usadas en el modelo: ")
cat(variables, sep=", ")
```

# ANEXO

### Código